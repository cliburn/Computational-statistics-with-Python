{
 "metadata": {
  "name": "",
  "signature": "sha256:a4c477657b04acda7a6aa983bc4a22f6f503824c8860c7359ac15fc520894312"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%matplotlib inline\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "%precision 4\n",
      "import os, sys, glob"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from numbapro import check_cuda, void, int32, int64, float32, float64\n",
      "from numbapro import cuda, jit, autojit, vectorize, guvectorize \n",
      "import timeit"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 22
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "check_cuda()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "------------------------------libraries detection-------------------------------\n",
        "Finding cublas\n",
        "\tlocated at /home/cliburn/anaconda/lib/libcublas.so.5.5.22\n",
        "\ttrying to open library...\tok"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Finding cusparse\n",
        "\tlocated at /home/cliburn/anaconda/lib/libcusparse.so.5.5.22\n",
        "\ttrying to open library...\tok"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Finding cufft\n",
        "\tlocated at /home/cliburn/anaconda/lib/libcufft.so.5.5.22\n",
        "\ttrying to open library...\tok"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Finding curand\n",
        "\tlocated at /home/cliburn/anaconda/lib/libcurand.so.5.5.22\n",
        "\ttrying to open library...\tok\n",
        "Finding nvvm\n",
        "\tlocated at /home/cliburn/anaconda/lib/libnvvm.so.2.0.0\n",
        "\ttrying to open library...\tok"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\tfinding libdevice for compute_20...\tok\n",
        "\tfinding libdevice for compute_30...\tok\n",
        "\tfinding libdevice for compute_35...\tok\n",
        "-------------------------------hardware detection-------------------------------\n",
        "Found 2 CUDA devices\n",
        "id 0       GeForce GTX TITAN                              [SUPPORTED]\n",
        "                      compute capability: 3.5\n",
        "                           pci device id: 0\n",
        "                              pci bus id: 3\n",
        "id 1         GeForce GTX 480                              [SUPPORTED]\n",
        "                      compute capability: 2.0\n",
        "                           pci device id: 0\n",
        "                              pci bus id: 4\n",
        "Summary:\n",
        "\t2/2 devices are supported\n",
        "PASSED\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 3,
       "text": [
        "True"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "@vectorize(['float32(float32, float32, float32)',\n",
      "            'float64(float64, float32, float64)'],\n",
      "           target='cpu')\n",
      "def add(a, b, c):\n",
      "    return a + b * c"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "@vectorize(['float32(float32, float32, float32)',\n",
      "            'float64(float64, float32, float64)'],\n",
      "           target='parallel')\n",
      "def mc_add(a, b, c):\n",
      "    return a + b * c"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "@vectorize(['float32(float32, float32, float32)',\n",
      "            'float64(float64, float64, float64)'],\n",
      "           target='gpu')\n",
      "def cu_add(a, b, c):\n",
      "    return a + b * c"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "dtype = np.float32\n",
      "for N in [1e5, 1e6, 1e7, 1e8]:\n",
      "    A = np.arange(N, dtype=dtype)\n",
      "    B = 10 * np.ones(N, dtype=dtype)\n",
      "    C = np.arange(N, dtype=dtype)\n",
      "    print '\\tN=%.0e\\tCPU:\\t' % N,\n",
      "    %timeit -n 1 D = add(A, B, C);\n",
      "    print '\\tN=%.0e\\tPAR:\\t' % N,\n",
      "    %timeit -n 1 D = mc_add(A, B, C);\n",
      "    print '\\tN=%.0e\\tGPU:\\t' % N,\n",
      "    %timeit -n 1 cu_D = cu_add(A, B, C);\n",
      "    print"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\tN=1e+05\tCPU:\t1 loops, best of 3: 76.1 \u00b5s per loop\n",
        "\tN=1e+05\tPAR:\t1 loops, best of 3: 91.1 \u00b5s per loop\n",
        "\tN=1e+05\tGPU:\t1 loops, best of 3: 2.34 ms per loop\n",
        "\n",
        "\tN=1e+06\tCPU:\t1 loops, best of 3: 1.75 ms per loop\n",
        "\tN=1e+06\tPAR:\t1 loops, best of 3: 1.29 ms per loop\n",
        "\tN=1e+06\tGPU:\t1 loops, best of 3: 5.91 ms per loop"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "\tN=1e+07\tCPU:\t"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "1 loops, best of 3: 24.4 ms per loop\n",
        "\tN=1e+07\tPAR:\t1 loops, best of 3: 12 ms per loop\n",
        "\tN=1e+07\tGPU:\t1 loops, best of 3: 46.6 ms per loop"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "\tN=1e+08\tCPU:\t"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "1 loops, best of 3: 240 ms per loop\n",
        "\tN=1e+08\tPAR:\t1 loops, best of 3: 116 ms per loop"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\tN=1e+08\tGPU:\t1 loops, best of 3: 417 ms per loop"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "@guvectorize(['void(float32[:,:], float32[:,:], float32[:,:])'],\n",
      "             '(m,n),(n,p)->(m,p)', target='cpu')\n",
      "def matmul(A, B, C):\n",
      "    m, n = A.shape\n",
      "    n, p = B.shape\n",
      "    for i in range(m):\n",
      "        for j in range(p):\n",
      "            C[i, j] = 0\n",
      "            for k in range(n):\n",
      "                C[i, j] += A[i, k] * B[k, j]\n",
      "\n",
      "w = 2\n",
      "A = np.arange(w**2).reshape(w, w).astype(dtype)\n",
      "B = np.arange(w**2).reshape(w, w).astype(dtype)\n",
      "C = matmul(A, B)\n",
      "print(\"A:\\n%s\" % A)\n",
      "print(\"B:\\n%s\" % B)\n",
      "print(\"C:\\n%s\" % C)\n",
      "%timeit matmul(A, B)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "A:\n",
        "[[ 0.  1.]\n",
        " [ 2.  3.]]\n",
        "B:\n",
        "[[ 0.  1.]\n",
        " [ 2.  3.]]\n",
        "C:\n",
        "[[  2.   3.]\n",
        " [  6.  11.]]\n",
        "100000 loops, best of 3: 2.39 \u00b5s per loop"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "@guvectorize(['void(float32[:,:], float32[:,:], float32[:,:])'],\n",
      "             '(m,n),(n,p)->(m,p)', target='gpu')\n",
      "def cu_matmul(A, B, C):\n",
      "    m, n = A.shape\n",
      "    n, p = B.shape\n",
      "    for i in range(m):\n",
      "        for j in range(p):\n",
      "            C[i, j] = 0\n",
      "            for k in range(n):\n",
      "                C[i, j] += A[i, k] * B[k, j]\n",
      "                \n",
      "cu_matmul.max_blocksize = 512 \n",
      "\n",
      "w = 2\n",
      "A = np.arange(w**2).reshape(w, w).astype(dtype)\n",
      "B = np.arange(w**2).reshape(w, w).astype(dtype)\n",
      "C = cu_matmul(A, B)\n",
      "print(\"A:\\n%s\" % A)\n",
      "print(\"B:\\n%s\" % B)\n",
      "print(\"C:\\n%s\" % C)\n",
      "%timeit cu_matmul(A, B)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "A:\n",
        "[[ 0.  1.]\n",
        " [ 2.  3.]]\n",
        "B:\n",
        "[[ 0.  1.]\n",
        " [ 2.  3.]]\n",
        "C:\n",
        "[[  2.   3.]\n",
        " [  6.  11.]]\n",
        "100 loops, best of 3: 2.25 ms per loop"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### CBE023"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "@cuda.jit('void()')\n",
      "def kernel():\n",
      "    pass\n",
      "\n",
      "kernel[1,1]()\n",
      "print \"Hello, World\\n\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Hello, World\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### CBE025"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "@vectorize('int64(int64, int64)')\n",
      "def add(a, b):\n",
      "    return a + b\n",
      "    \n",
      "add(2, 7)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 23,
       "text": [
        "9"
       ]
      }
     ],
     "prompt_number": 23
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### Get info"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "gpu = cuda.get_current_device()\n",
      "gpu.compute_capability"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 24,
       "text": [
        "(3, 5)"
       ]
      }
     ],
     "prompt_number": 24
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def total_cores(gpu):\n",
      "    cores_per_capability = {\n",
      "    1: 8,\n",
      "    2: 32,\n",
      "    3: 192,\n",
      "    }\n",
      "    major, minor = gpu.compute_capability\n",
      "    return cores_per_capability[major] * gpu.MULTIPROCESSOR_COUNT \n",
      "\n",
      "total_cores(gpu)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 27,
       "text": [
        "2688"
       ]
      }
     ],
     "prompt_number": 27
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}