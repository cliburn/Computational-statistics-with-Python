{
 "metadata": {
  "name": "",
  "signature": "sha256:b8f8919752c0fd645fca625c521106e7fb799cd8fcf9d98f8b6167f549a6a7fb"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Linear Algebra and Linear Systems"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "A lot of problems in statistical computing can be described mathematically using linear algebra.  This lecture is meant to serve as a review of concepts you have covered in linear algebra courses."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Simultaneous Equations"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Consider a set of $m$ linear equations in $n$ unknowns:\n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "\\begin{align*}\n",
      "a_{11} x_1 + &a_{12} x_2& +& ... + &a_{1n} x_n &=& b_1\\\\\n",
      "\\vdots  && &&\\vdots &= &\\vdots\\\\\n",
      "a_{m1} x_1 + &a_{m2} x_2& +& ... + &a_{mn} x_n &=&b_m \n",
      "\\end{align*}\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%latex\n",
      "We can let:\n",
      "\n",
      "\\begin{align*}\n",
      "    A=\\left[\\begin{matrix}a_{11}&\\cdots&a_{1n}\\\\\n",
      "               \\vdots & &\\vdots\\\\\n",
      "               a_{m1}&\\cdots&a_{mn}\\end{matrix}\\right], & & \n",
      "\n",
      "x = \\left[\\begin{matrix}x_1\\\\\n",
      "               \\vdots\\\\\n",
      "               x_n\\end{matrix}\\right] & \\;\\;\\;\\;\\textrm{   and } &\n",
      "b =  \\left[\\begin{matrix}b_1\\\\\n",
      "               \\vdots\\\\\n",
      "               b_m\\end{matrix}\\right]\n",
      "\\end{align*}\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "latex": [
        "We can let:\n",
        "\n",
        "\\begin{align*}\n",
        "    A=\\left[\\begin{matrix}a_{11}&\\cdots&a_{1n}\\\\\n",
        "               \\vdots & &\\vdots\\\\\n",
        "               a_{m1}&\\cdots&a_{mn}\\end{matrix}\\right], & & \n",
        "\n",
        "x = \\left[\\begin{matrix}x_1\\\\\n",
        "               \\vdots\\\\\n",
        "               x_n\\end{matrix}\\right] & \\;\\;\\;\\;\\textrm{   and } &\n",
        "b =  \\left[\\begin{matrix}b_1\\\\\n",
        "               \\vdots\\\\\n",
        "               b_m\\end{matrix}\\right]\n",
        "\\end{align*}\n"
       ],
       "metadata": {},
       "output_type": "display_data",
       "text": [
        "<IPython.core.display.Latex at 0x7f98bc0eb950>"
       ]
      }
     ],
     "prompt_number": 22
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "And re-write the system:\n",
      "    \n",
      "$$ Ax = b$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Solving the system now amounts to finding $A^{-1}$.  Of course, strictly speaking, $A^{-1}$ is defined only when $A$ is an $n\\times n$ matrix of full rank.  There are weaker forms of inverse (generalized inverses) that work in the case that $A$ is $m\\times n$.  A generalized inverse of $A$, $A^g$, is a matrix with the property:  "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "$$A A^g A = A$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The generalized inverse is not unique.  When it exists, the Moore-Penrose inverse is useful.  It has the additional properties:\n",
      "\n",
      "$$A^g A A^g = A^g$$\n",
      "$$\\left(A A^g\\right)^t = A A^g$$\n",
      "$$\\left(A^g A\\right)^t = A^g A$$\n",
      "\n",
      "In other words, $A$ is also a generalized inverse of $A^g$, and the products $A A^g$ and $A^g A$ are symmetric."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "If $A^g$ is the Moore-Penrose inverse of $A$, then \n",
      "$$x = A^g b$$\n",
      "is the shortest length least-squares solution to\n",
      "$$Ax =b$$."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "So, lets solve a system in Python!"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "from scipy import linalg\n",
      "from sympy import init_printing\n",
      "init_printing() \n",
      "\n",
      "np.set_printoptions(suppress=True)   # Suppress printing of negligibly small numbers - print as zero.\n",
      "a = np.random.randn(6, 9)            # generate a 6x9 random matrix\n",
      "a_pseudo_inv = np.linalg.pinv(a)\n",
      "print(np.dot(a,a_pseudo_inv))                  # We can see that a_pseudo_inv has inverted part of a.\n",
      "\n",
      "b = np.random.rand(6,1)\n",
      "\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[[ 1. -0.  0.  0.  0.  0.]\n",
        " [-0.  1.  0.  0. -0.  0.]\n",
        " [ 0. -0.  1.  0.  0. -0.]\n",
        " [-0. -0.  0.  1. -0. -0.]\n",
        " [ 0.  0. -0. -0.  1. -0.]\n",
        " [ 0. -0. -0.  0. -0.  1.]]\n"
       ]
      }
     ],
     "prompt_number": 33
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The solution to the system $Ax=b$ is given by:\n",
      "    $$A^gb +(I - A^gA)v$$\n",
      "where $v$ is an arbitrary vector.  If $A$ is invertible, then $A^g$ is its unique inverse and $I-A^gA$ is the zero matrix."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(np.dot(a_pseudo_inv,b)) \n",
      "print(np.identity(9) - np.dot(a_pseudo_inv, a))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[[-0.12841516]\n",
        " [-0.36336625]\n",
        " [ 0.60058422]\n",
        " [ 0.00486597]\n",
        " [-0.28601928]\n",
        " [-0.1960057 ]\n",
        " [-0.13937247]\n",
        " [-0.06282456]\n",
        " [ 0.51636128]]\n",
        "[[ 0.36682917 -0.29556611 -0.00798435 -0.07323267 -0.26318511  0.10394593\n",
        "   0.17435374  0.06246794 -0.15845125]\n",
        " [-0.29556611  0.29561398  0.02899319 -0.06616848  0.19651993  0.01509987\n",
        "  -0.18568564  0.10250925  0.17836131]\n",
        " [-0.00798435  0.02899319  0.32398083 -0.29292362  0.15730165  0.02202679\n",
        "  -0.10594499 -0.08154782 -0.29867252]\n",
        " [-0.07323267 -0.06616848 -0.29292362  0.47844838 -0.03645857 -0.2247616\n",
        "   0.12941367 -0.23616962  0.1721017 ]\n",
        " [-0.26318511  0.19651993  0.15730165 -0.03645857  0.27196026 -0.1085708\n",
        "  -0.15512565 -0.15632339 -0.06123531]\n",
        " [ 0.10394593  0.01509987  0.02202679 -0.2247616  -0.1085708   0.20016633\n",
        "  -0.02446095  0.28707625  0.05714297]\n",
        " [ 0.17435374 -0.18568564 -0.10594499  0.12941367 -0.15512565 -0.02446095\n",
        "   0.14104672 -0.05296395 -0.02888653]\n",
        " [ 0.06246794  0.10250925 -0.08154782 -0.23616962 -0.15632339  0.28707625\n",
        "  -0.05296395  0.47954415  0.25117738]\n",
        " [-0.15845125  0.17836131 -0.29867252  0.1721017  -0.06123531  0.05714297\n",
        "  -0.02888653  0.25117738  0.44241018]]\n"
       ]
      }
     ],
     "prompt_number": 24
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "There are many techniques to solve linear systems.  Our goal is to understand the theory behind many of the built-in functions, and how they *efficiently* solve systems of equations.  In fact, finding $A^{-1}$ is often *not* the best approach to solving a system of equations.\n",
      "\n",
      "\n",
      "First, let's review some linear algebra topics:"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Linear Independence"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "A collection of vectors $v_1,...,v_n$ is said to be *linearly independent* if\n",
      "\n",
      "$$c_1v_1 + \\cdots c_nv_n = 0$$\n",
      "$$\\iff$$\n",
      "$$c_1=\\cdots=c_n=0$$\n",
      "\n",
      "In other words, any linear combination of the vectors that results in a zero vector is trivial."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Another interpretation of this is that no vector in the set may be expressed as a linear combination of the others.  In this sense, linear independence is an expression of non-redundancy in a set of vectors.\n",
      "\n",
      "\n",
      "Fact: Any linearly independent set of $n$ vectors spans an $n$-dimensional space. (I.e. the collection of all possible linear combinations is $\\mathbb{R}^n$.)  Such a set of vectors is said to be a *basis* of $\\mathbb{R}^n$.  Another term for basis is *minimal spanning set*."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Norms and Distance of Vectors"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Recall that the 'norm' of a vector $v$, denoted $||v||$ is simply its length.  For a vector with components \n",
      "$$v = \\left(v_1,...,v_n\\right)$$\n",
      "the norm of $v$ is given by:\n",
      "    \n",
      "$$||v|| = \\sqrt{v_1^2+...+v_n^2}$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The distance between two vectors is the length of their difference:\n",
      "    \n",
      "$$d(v,w) = ||v-w||$$"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Examples"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# norm of a vector\n",
      "\n",
      "v = np.array([1,2])\n",
      "linalg.norm(v)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "latex": [
        "$$2.2360679775$$"
       ],
       "metadata": {},
       "output_type": "pyout",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAHUAAAASCAYAAABl9ZFeAAAABHNCSVQICAgIfAhkiAAABP9JREFU\naIHt2WmoVVUUB/Dfc+hlpVlqvqRIy2aoXoOGiUQTSX2IPlSQZQNkA1FQUUSDZVYIjSRGH6IyTNAo\nGqCBrAwatLTSEq3Qykgzmi0z0z6sc3r7Hc/tnnMrgnh/uDz3Ov/1P/fuvfZaa2/pwf8ObYXxKFyD\nftgNC3ADvqigVdX3SEzCLxl3O9yK90o098HkjPtr9ncyfkw4++I2fIotGIyrsLYFvfG4P/suOWdz\n4r8Q07N/74kbsQG/Y3tcjTUJv6penfcuxxS8jPU4XMz7pVhW/MGH4gUMzMY7YD6+wvAiuUXfTjyD\nbRPbDPyAQwqaB+AzjMnGHVgpFiHHjliNCYntWizFNi3oXSECo9FnfMYbga9xauI7QSxKnxb0qvKU\nPNuISzTAsxhZsHVmjrMbOdX0vSuznZ7YTs5s9ya2PiIiL09su2MdLktsU0XgpBO5M37DRS3ozcjs\nfdErsR+F+5Lxk/imwOkndu05LehV5REZ6QE8gWnY31/gJxHJuxTs34qo/Cd8z8b3OD6xnSEW9fbE\ndp6IwIH+GivwdIl9Cea1oFecQCLrPCfKBJEBfhPlpYjlIhPV0avDg1dKuA2xROTyEQX7GpG7/y3f\nadiEgxPbi/iwiV9/EQzTS549L4Knjl4jzMDoZNyRvXd+CXeByBx19OryXmnmmKatI8VEpYV+GIZW\nEGrVdwQm4mJdjVIbxooJGocTRNQOx01YnPH2yP7+UKK7HgPQLnZoFb0yHIXeeCuxrcPPuvcFOYZh\niJjXTRX1qr43RzuuxyCRMfYSjdKKJpp/4nbR2Y1pRqzpe7Lo4N4XjU1aRwaLnbAMFyb2o0WXemA2\nHpPxbirRn5k9G1pDrwzv2Dr7wIOirKSnhw7RsW6xdRlqpleH97GovzkmiE6/o4KukaJW3lKF3KJv\nH5Ea3xSTTyzEFtF09CvwV+uqWaMz3uQS3dnZszxbVNEr4liNU/YQsTMmJb/jFizK3jWopl4dXq/C\nuLfIHPeWcLuhXaSrOyt8ib/rO05MxNxs3DcbLy3hvilqd7s4JzZa1KezZ/1r6BXxuNiRjbCTOKfe\nKRZ0D7wtzpnF838Vvbq8FJ9mn4ZowyzcXFO4iu9+OKhgGyAmfbOodUSz8VqJ/6sZd1dx2N+Me0p4\n80TnnaOKXoq+ItNMbfA7GuFzvFRir6rXjDdf+e9YLTIRtt7KRK1bJm6Dcpzd5MtU8R0gmpJForjn\n+D372yZSCd3TcYp2sbPWiWZose71JcdIvJuMq+ilGCWCptlRLsUQcZM2p+RZVb1mvE5dgZ9isGSn\nFhf1XBH9Uwr2sYXx3rauT818N4ras1L3XZQfnhfoOoY8JtJZ+o42sdOf0tVZPitqa5ru9hILPTex\nVdXLkV+kbFCOy8Xu3y2xTRRXog+V8JvpVeU9h+MKtk4RnI+WORwjIuTRwme2mJQc48TiPd+C763i\njjJdhJmiCz00sfXCG7gysZ0mdtTwxLYrvsNZie1ufKD7NWFVvRxXibR8fskzuA6rdHWcnaIDbXRK\naKZXlXeECNb8QqINj+B1SV+QnlMfFzcuZ5aIpV3sWjEZn7Tge624RpsldkeHuG47TPdz1macJJqQ\nOeI81ibOw6sS3pfiaDJVBEV/cU14osgMdfVyfCSyRtl/MsAd4t75tuydO+IUEThlaKZXlbdQ9BAP\ni4ZsgLj4uUCUkR70oAc96MF/hz8ARsGd0oeD8pUAAAAASUVORK5CYII=\n",
       "prompt_number": 36,
       "text": [
        "2.2360679775"
       ]
      }
     ],
     "prompt_number": 36
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# distance between two vectors\n",
      "\n",
      "w = np.array([1,1])\n",
      "linalg.norm(v-w)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "latex": [
        "$$1.0$$"
       ],
       "metadata": {},
       "output_type": "pyout",
       "png": "iVBORw0KGgoAAAANSUhEUgAAABsAAAASCAYAAACq26WdAAAABHNCSVQICAgIfAhkiAAAAS5JREFU\nOI3t1L8rhXEUx/GX37lycZOSZOAmKQPJJqM/gj/AH2Cxy+oPMNySMth0B4tZGSgsykCiyyKykB/D\n8731qOdxXT2ZnOV0Pqdz3t+fhz+0hhQ9hwNM1NFrFGu4xAd6sYzKd0XTOAwFP7UuXGMhpq3gFK1J\nBWMooyTaVT2wVdyhOaYV8IqlWsWlOmHn2E3QT7BfDRrraJhmnSjiKiF3g6ksYUPBPybknpFHW1aw\nfPAvKTDozgr2FnzSHbcE35QV7P6bXEfwT1nBKqJd9aTAHrKEPeMIgwm5ERxXg9/CimiPxWXM+Dr+\nhsMCdmo12xYdTS4hN4t37MW0ftFxLca0dZyJjav4eOnDJgYwHrRL0XzbwFbQKqJHcRGrvcWcaGxN\nij56AfOSv8S//d4+AbVGOATlzlzwAAAAAElFTkSuQmCC\n",
       "prompt_number": 38,
       "text": [
        "1.0"
       ]
      }
     ],
     "prompt_number": 38
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Inner Products"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Inner products are closely related to norms and distance.  The (standard) inner product of two $n$ dimensional vectors $v$ and $w$ is given by:\n",
      "\n",
      "$$<v,w> = v_1w_1+...+v_nw_n$$\n",
      "\n",
      "I.e. the inner product is just the sum of the product of the components.  Certain 'special' matrices also define inner products, and we will see some of those later.\n",
      "\n",
      "Any inner product determines a norm via:\n",
      "\n",
      "$$||v|| = <v,v>^{\\frac12}$$"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Example"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "v.dot(w)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "latex": [
        "$$3$$"
       ],
       "metadata": {},
       "output_type": "pyout",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAAwAAAASCAYAAABvqT8MAAAABHNCSVQICAgIfAhkiAAAAOFJREFU\nKJHN0q1Ow0EQBPAfpIKvFIPAEkwxQFGoWkw9FgWKF2hSRVKBIanpA9QicbwAwRCQNWgMBEhoQgit\n+F/p9XKhxTFmc7M7e3N7yx8xl5z3cYI+FrGEFu5z4iqusBBxHbxhNye4wACHEVcPXHtEzEfJu9Dt\nOeJWQvzI3ZDDOb6wM0vxBp5wPK2wjjM8oGHS9q8o4Ro3WJtVVFNM6TKXrGA74cpB8G08sZ9EXzGR\nzYhfDoIBVhk/6DN4fsRLJNgK8RavqaUWTk3uVxfv2BsR6fId4SBYW1f8ehO9tPs/whD8Cif7+fal\n4gAAAABJRU5ErkJggg==\n",
       "prompt_number": 40,
       "text": [
        "3"
       ]
      }
     ],
     "prompt_number": 40
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Outer Products"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Note that the inner product is just matrix multiplication of a $1\\times n$ vector with an $n\\times 1$ vector.  In fact, we may write:\n",
      "\n",
      "$$<v,w> = v^tw$$\n",
      "\n",
      "The *outer product* of two vectors is just the opposite. It is given by:\n",
      "\n",
      "$$v\\otimes w = vw^t$$\n",
      "\n",
      "Note that I am considering $v$ and $w$ as *column* vectors.  The result of the inner product is a *scalar*. The result of the outer product is a *matrix*."
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Example"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "np.outer(v,w)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 42,
       "text": [
        "array([[1, 1],\n",
        "       [2, 2]])"
       ]
      }
     ],
     "prompt_number": 42
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Trace and Determinant of Matrices"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The trace of a matrix $A$ is the sum of its diagonal elements.  It is important for a couple of reasons:\n",
      "\n",
      "* It is an *invariant* of a matrix under change of basis (more on this later).\n",
      "* It defines a matrix norm (more on that later)\n",
      "\n",
      "The determinant of a matrix is defined to be the alternating sum of permutations of the elements of a matrix.  Let's not dwell on that though. It is important to know that the determinant of a $2\\times 2$ matrix is\n",
      "\n",
      "$$\\left|\\begin{matrix}a_{11} & a_{12}\\\\a_{21} & a_{22}\\end{matrix}\\right| = a_{11}a_{22} - a_{12}a_{21}$$\n",
      "\n",
      "This may be extended to an $\\times n$ matrix by minor expansion.  I will leave that for you to google.  We will be computing determinants using tools such as:\n",
      "\n",
      "``np.linalg.det(A)``\n",
      "\n",
      "What is most important about the determinant:\n",
      "\n",
      "* Like the trace, it is also invariant under change of basis\n",
      "* An $n\\times n$ matrix $A$ is invertible $\\iff$ det$(A)\\neq 0$ "
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Column space, Row space, Rank and Kernel"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let $A$ be an $m\\times n$ matrix.  We can view the columns of $A$ as vectors, say $\\textbf{a_1},...,\\textbf{a_n}$. The space of all linear combinations of the $\\textbf{a_i}$ are the *column space* of the matrix $A$.  Now, if $\\textbf{a_1},...,\\textbf{a_n}$ are *linearly independent*, then the column space is of dimension $n$.  Otherwise, the dimension of the column space is the size of the maximal set of linearly independent $\\textbf{a_i}$.  Row space is exactly analogous, but the vectors are the *rows* of $A$.\n",
      "\n",
      "The *rank* of a matrix *A* is the dimension of its column space - and - the dimension of its row space.  These are equal for any matrix.  Rank can be thought of as a measure of non-degeneracy of a system of linear equations, in that it is the *dimension of the image of the linear transformation* determined by $A$. \n",
      "\n",
      "The *kernel* of a matrix *A* is the dimension of the space mapped to zero under the linear transformation that $A$ represents. The dimension of the kernel of a linear transformation is called the *nullity*. \n",
      "\n",
      "Index theorem: For and $m\\times n$ matrix $A$, \n",
      "\n",
      "rank($A$) + nullity($A$) = $n$.\n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Matrices as Linear Transformations"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Matrices are not just nice ways to package systems of linear equations.  In fact, they have properties that facilitate solving such systems.\n",
      "\n",
      "Let's consider: what does a matrix *do* to a vector?  Matrix multiplication has a *geometric* interpretation.  When we multiply a vector, we either rotate, reflect, dilate or some combination of those three. So multiplying by a matrix *transforms* one vector into another vector.  This is known as a *linear transformation*.\n",
      "\n",
      "Important Facts: \n",
      "\n",
      "* Any matrix defines a linear transformation\n",
      "* The matrix form of a linear transformation is NOT unique\n",
      "* We need only define a transformation by saying what it does to a *basis*\n",
      "\n",
      "Suppose we have a matrix $A$ that defines some transformation.  We can take any invertible matrix $B$ and\n",
      "\n",
      "$$BAB^{-1}$$\n",
      "\n",
      "defines the same transformation.  This operation is called a *change of basis*, because we are simply expressing the transformation with respect to a different basis.  "
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Example:\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let $f(x)$ be the linear transformation that takes $e_1=(1,0)$ to $f(e_1)=(2,3)$ and $e_2=(0,1)$ to $f(e_2) = (1,1)$.  A matrix representation of $f$ would be given by:\n",
      "\n",
      "$$A = \\left(\\begin{matrix}2 & 1\\\\3&1\\end{matrix}\\right)$$\n",
      "\n",
      "This is the matrix we use if we consider the vectors of $\\mathbb{R}^2$ to be linear combinations of the form \n",
      "\n",
      "$$c_1 e_1 + c_2 e_2$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now, consider a second pair of (linearly independent) vectors in $\\mathbb{R}^2$, say $v_1=(1,3)$ and $v_2=(4,1)$. We first find the transformation that takes $e_1$ to $v_1$ and $e_2$ to $v_2$.  A matrix representation for this is:\n",
      "\n",
      "$$B = \\left(\\begin{matrix}1 & 4\\\\3&1\\end{matrix}\\right)$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Our original transformation $f$ can be expressed with respect to the basis $v_1, v_2$ via\n",
      "\n",
      "$$B^{-1}AB$$"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "\n",
      "A = np.array([[2,1],[3,1]])  # transformation f in standard basis\n",
      "e1 = np.array([1,0])         # standard basis vectors e1,e2\n",
      "e2 = np.array([0,1])\n",
      "\n",
      "print(A.dot(e1))             # demonstrate that Ae1 is (2,3)\n",
      "print(A.dot(e2))             # demonstrate that Ae2 is (1,1) \n",
      "                              \n",
      "# new basis vectors\n",
      "v1 = np.array([1,3])         \n",
      "v2 = np.array([4,1])\n",
      "\n",
      "# How v1 and v2 are transformed by A\n",
      "print(\"Av1: \")\n",
      "print(A.dot(v1))   \n",
      "print(\"Av2: \")\n",
      "print(A.dot(v2))\n",
      "\n",
      "# Change of basis from standard to v1,v2\n",
      "B = np.array([[1,4],[3,1]])\n",
      "B_inv = linalg.inv(B)\n",
      "\n",
      "print(\"B B_inv \")\n",
      "print(B.dot(B_inv))   # check inverse\n",
      "\n",
      "# transform e1 under change of coordinates\n",
      "T = B_inv.dot(A.dot(B))        # B^{-1} A B  \n",
      "coeffs = T.dot(e1)\n",
      "\n",
      "print(\"Compare to Av1: \", coeffs[0]*v1 + coeffs[1]*v2)\n",
      "\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[2 3]\n",
        "[1 1]\n",
        "Av1: \n",
        "[5 6]\n",
        "Av2: \n",
        "[ 9 13]\n",
        "B B_inv \n",
        "[[ 1.  0.]\n",
        " [ 0.  1.]]\n",
        "('Compare to Av1: ', array([ 5.,  6.]))\n"
       ]
      }
     ],
     "prompt_number": 32
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Special Matrices"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Some matrices have interesting properties that allow us either simplify the underlying linear system or to understand more about it. "
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Square Matrices"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Square matrices have the same number of columns (usually denoted $n$).  We refer to an arbitrary square matrix as and $n\\times n$ or we refer to it as a 'square matrix of dimension $n$'.  If an $n\\times n$ matrix $A$ has *full rank* (i.e. it has rank $n$), then $A$ is invertible, and its inverse is unique.  This is a situation that leads to a unique solution to a linear system."
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Diagonal Matrices"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "A diagonal matrix is a matrix with all entries off the diagonal equal to zero.  Strictly speaking, such a matrix should be square, but we can also consider rectangular matrices of size $m\\times n$ to be diagonal, if all entries $a_{ij}$ are zero for $i\\neq j$"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Symmetric and Skew Symmetric"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "A matrix $A$ is (skew) symmetric iff $a_{ij} = (-)a_{ji}$.\n",
      "\n",
      "Equivalently, $A$ is (skew) symmetric iff\n",
      "\n",
      "$$A = (-)A^T$$"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Upper and Lower Triangular"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "A matrix $A$ is (upper|lower) triangular if $a_{ij} = 0$ for all $i (>|<) j$"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Orthogonal and Orthonormal"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "A matrix $A$ is *orthogonal* iff\n",
      "\n",
      "$$A A^T = I$$\n",
      "\n",
      "In other words, $A$ is orthogonal iff \n",
      "\n",
      "$$A^T=A^{-1}$$\n",
      "\n",
      "Fact: The rows and columns of an orthogonal matrix are an orthonormal set of vectors.\n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Positive Definite"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Positive definite matrices are an important class of matrices with very desirable properties. A square matrix $A$ is positive definite if\n",
      "\n",
      "$$u^TA u > 0$$\n",
      "\n",
      "for any non-zero n-dimensional vector $u$.\n",
      "\n",
      "A symmetric, positive-definite matrix $A$ is a positive-definite matrix such that\n",
      "\n",
      "$$A = A^T$$\n",
      "\n",
      "IMPORTANT: \n",
      "\n",
      "* Symmetric, positive-definite matrices have 'square-roots' (in a sense)\n",
      "* Any symmetric, positive-definite matrix is *diagonizable*!!!\n",
      "* Co-variance matrices are symmetric and positive-definite\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}