{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "%precision 4\n",
    "plt.style.use('ggplot')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from IPython.display import HTML, Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Writing CUDA in C\n",
    "===="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review of GPU Architechture - A Simplification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPUs or GPGPUs are complex devices, but to get started, one really just needs to understand a more simplistic view."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "![GPUs and CPUs](./GPUCPUOverview.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most important thing to understand about memory, is that the CPU can access both main memory (host) and GPU memory (device).  The device sees only its memory, and cannot access the host memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kernels, Threads and Blocks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that GPUs are SIMD.  This means that each CUDA core gets the same code, called a 'kernel'.  Kernels are programmed to execute one 'thread' (execution unit or task).  The 'trick' is that each thread 'knows' its identity, in the form of a grid location, and is usually coded to access an array of data at a unique location for the thread.  \n",
    "\n",
    "We will concentrate on a 1-dimensional grid with each thread in a block by itself, but let's understand when we might want to organize threads into blocks.\n",
    "\n",
    "\n",
    "GPU memory can be expanded (roughly) into 3 types:\n",
    "\n",
    "* local - memory only seen by the thread. This is the fastest type\n",
    "* shared - memory that may be seen by all threads in a block.  Fast memory, but not as fast as local.\n",
    "* global - memory seen by all threads in all blocks.  This is the slowest to access.\n",
    "\n",
    "So, if multiple threads need to use the *same* data (not unique chunks of an array, but the very same data), then those threads should be grouped into a common block, and the data should be stored in shared memory.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cuda C program - an Outline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following are the minimal ingredients for a Cuda C program:\n",
    "\n",
    "* The kernel.  This is the function that will be executed in parallel on the GPU.\n",
    "\n",
    "\n",
    "* Main C program\n",
    "  - allocates memory on the GPU\n",
    "  - copies data in CPU memory to GPU memory\n",
    "  - 'launches' the kernel (just a function call with some extra arguments)\n",
    "  - copies data from GPU memory back to CPU memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kernel Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting kernel.hold\n"
     ]
    }
   ],
   "source": [
    "%%file kernel.hold\n",
    "\n",
    "__global void square_kernel(float *d_out, float *d_in){\n",
    "  \n",
    "  int i = thread.Idx;   # This is a unique identifier of the thread   \n",
    "  float f = d_in[i]     # Why this statement?\n",
    "  d_out[i] = f*f;       # d_out is what we will copy back to the host memory\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CPU Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting main.hold\n"
     ]
    }
   ],
   "source": [
    "%%file main.hold\n",
    "\n",
    "int main(int argc, char **argv){\n",
    "const int ARRAY_SIZE = 64;\n",
    "    const int ARRAY_BYTES = ARRAY_SIZE * sizeof(float);\n",
    "\n",
    "    float h_in[ARRAY_SIZE];\n",
    "\n",
    "    for (int i =0;i<ARRAY_SIZE;i++){\n",
    "        h_in[i] = float(i);\n",
    "    } float h_out[ARRAY_SIZE];\n",
    "\n",
    "    float *d_in;  // These are device memory pointers\n",
    "    float *d_out;\n",
    "\n",
    "    cudaMalloc((void **) &d_in, ARRAY_BYTES);\n",
    "    cudaMalloc((void **) &d_out, ARRAY_BYTES);\n",
    "      \n",
    "    cudaMemcpy(d_in, h_in, ARRAY_BYTES,cudaMemcpyHostToDevice);\n",
    "\n",
    "    square_kernel<<<1,ARRAY_SIZE>>>(d_out,d_in);\n",
    "\n",
    "    cudaMemcpy(h_out,d_out,ARRAY_BYTES,cudaMemcpyDeviceToHost);\n",
    "\n",
    "    for (int i = 0;i<ARRAY_SIZE;i++){\n",
    "        printf(\"%f\", h_out[i]);\n",
    "        printf(((i % 4) != 3 ? \"\\t\" : \"\\n\"));\n",
    "    }\n",
    "   \n",
    "    cudaFree(d_in);\n",
    "\n",
    "\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shared Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lifted from: https://www.cac.cornell.edu/vw/gpu/shared_mem_exec.aspx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting shared_mem_ex.cu\n"
     ]
    }
   ],
   "source": [
    "%%file shared_mem_ex.cu\n",
    "\n",
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "\n",
    "#define N 1024*1024\n",
    "#define BLOCKSIZE 1024\n",
    "\n",
    "__global__ \n",
    "void share_ary_oper(int *ary, int *ary_out)\n",
    "{\n",
    "    // Thread index\n",
    "        int tx = threadIdx.x;\n",
    "        int idx=blockDim.x*blockIdx.x + threadIdx.x;\n",
    "        __shared__ int part_ary[BLOCKSIZE];\n",
    "\n",
    "        part_ary[tx]=ary[idx];\n",
    "        part_ary[tx]=part_ary[tx]*10;\n",
    "        ary_out[idx]=part_ary[tx];\n",
    "        __syncthreads();\n",
    "}\n",
    "\n",
    "int main(){\n",
    "\n",
    "        int *device_array, *device_array_out;\n",
    "        int *host_array, *host_array_out;\n",
    "        int i, nblk;\n",
    "        float k;\n",
    "        size_t size = N*sizeof(int);\n",
    "\n",
    "//Device memory\n",
    "        cudaMalloc((void **)&device_array, size);\n",
    "        cudaMalloc((void **)&device_array_out, size);\n",
    "//Host memory\n",
    "//cudaMallocHost() produces pinned memoty on the host\n",
    "        cudaMallocHost((void **)&host_array, size);\n",
    "        cudaMallocHost((void **)&host_array_out, size);\n",
    "\n",
    "        for(i=0;i<N;i++)\n",
    "        {\n",
    "                host_array[i]=i;\n",
    "                host_array_out[i]=0;\n",
    "        }\n",
    "        cudaMemcpy(device_array, host_array, size, cudaMemcpyHostToDevice);\n",
    "        cudaMemcpy(device_array_out, host_array_out, size, cudaMemcpyHostToDevice);\n",
    "        nblk=N/BLOCKSIZE;\n",
    "        share_ary_oper<<<nblk, BLOCKSIZE>>>(device_array, device_array_out);\n",
    "        cudaMemcpy(host_array, device_array, size, cudaMemcpyDeviceToHost);\n",
    "        cudaMemcpy(host_array_out, device_array_out, size, cudaMemcpyDeviceToHost);\n",
    "\n",
    "\n",
    "\tprintf(\"Printing elements 10-15 of output array\\n\");\n",
    "        for (i=N;i<N;i++)\n",
    "        {\n",
    "                k=host_array_out[i]-i*10;    \n",
    "                if(k<0.1)\n",
    "                        printf(\"Incorrect IX %d=%.1f\\n\",i, k);\n",
    "        }\n",
    "        for (i=10;i<15;i++)\n",
    "                printf(\"host_array_out[%d]=%d\\n\", i, host_array_out[i]);\n",
    "\n",
    "        cudaFree(device_array);\n",
    "        cudaFree(host_array);\n",
    "        cudaFree(device_array_out);\n",
    "        cudaFree(host_array_out);\n",
    "        cudaDeviceReset();\n",
    "        return EXIT_SUCCESS;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Makefile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting Makefile\n"
     ]
    }
   ],
   "source": [
    "%%file Makefile\n",
    "\n",
    "CC=nvcc\n",
    "CFLAGS=-Wall\n",
    "\n",
    "shared_mem.o: shared_mem_ex.cu\n",
    "\t $(CC) $(CFAGS) -c shared_mem_ex.cu\n",
    "\n",
    "clean:\n",
    "\t rm -f *.o"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Compile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvcc  -c shared_mem_ex.cu\r\n"
     ]
    }
   ],
   "source": [
    "! make "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
