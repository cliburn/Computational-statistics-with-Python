{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark on a local mahcine using 4 nodes\n",
    "====\n",
    "\n",
    "Started with\n",
    "```bash\n",
    "SPARK_WORKER_MEMORY=512m MASTER=local[4] IPYTHON_OPTS=\"notebook\" pyspark\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have a Spark cluster, just set\n",
    "```bash\n",
    "MASTER=spark://IP:PORT\n",
    "```\n",
    "Everything else works the same way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Spark in standalone prograsm\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "conf = (SparkConf()\n",
    "         .setMaster(\"local[4]\")\n",
    "         .setAppName(\"STA663\")\n",
    "         .set(\"spark.executor.memory\", \"4g\"))\n",
    "sc = SparkContext(conf = conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check that the SparkContext object is available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.context.SparkContext at 0x10fecee10>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Spark concepts with a data manipulation example\n",
    "\n",
    "Adapted from scala version in Chapter 2: Introduction to Data Analysis with Scala and Spark of Advanced Analytics with Spark (O'Reilly 2015)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "10 archives were successfully processed.\r\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "if not os.path.exists('documentation'):\n",
    "    ! curl -o documentation https://archive.ics.uci.edu/ml/machine-learning-databases/00210/documentation\n",
    "if not os.path.exists('donation.zip'):\n",
    "    ! curl -o donation.zip https://archive.ics.uci.edu/ml/machine-learning-databases/00210/donation.zip\n",
    "! unzip -n -q donation.zip\n",
    "! unzip -n -q 'block_*.zip'\n",
    "if not os.path.exists('linkage'):\n",
    "    ! mkdir linkage\n",
    "! mv block_*.csv linkage\n",
    "! rm block_*.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Info about the data set\n",
    "\n",
    "Please see the `documentation` file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If we are running Spark on Hadoop, we need to transfer files to HDFS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "! hadoop fs -mkdir linkage\n",
    "! hadoop fs -put block_*.csv linkage\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rdd = sc.textFile('linkage')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Actions trigger execution and return a non-RDD result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'\"id_1\",\"id_2\",\"cmp_fname_c1\",\"cmp_fname_c2\",\"cmp_lname_c1\",\"cmp_lname_c2\",\"cmp_sex\",\"cmp_bd\",\"cmp_bm\",\"cmp_by\",\"cmp_plz\",\"is_match\"'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'\"id_1\",\"id_2\",\"cmp_fname_c1\",\"cmp_fname_c2\",\"cmp_lname_c1\",\"cmp_lname_c2\",\"cmp_sex\",\"cmp_bd\",\"cmp_bm\",\"cmp_by\",\"cmp_plz\",\"is_match\"',\n",
       " u'37291,53113,0.833333333333333,?,1,?,1,1,1,1,0,TRUE',\n",
       " u'39086,47614,1,?,1,?,1,1,1,1,1,TRUE',\n",
       " u'70031,70237,1,?,1,?,1,1,1,1,1,TRUE',\n",
       " u'84795,97439,1,?,1,?,1,1,1,1,1,TRUE',\n",
       " u'36950,42116,1,?,1,1,1,1,1,1,1,TRUE',\n",
       " u'42413,48491,1,?,1,?,1,1,1,1,1,TRUE',\n",
       " u'25965,64753,1,?,1,?,1,1,1,1,1,TRUE',\n",
       " u'49451,90407,1,?,1,?,1,1,1,1,0,TRUE',\n",
       " u'39932,40902,1,?,1,?,1,1,1,1,1,TRUE']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def is_header(line):\n",
    "    return \"id_1\" in line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transforms return an RDD and are lazy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[4] at RDD at PythonRDD.scala:42"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vals = rdd.filter(lambda x: not is_header(x))\n",
    "vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5749132"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vals.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now it is evaluated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'37291,53113,0.833333333333333,?,1,?,1,1,1,1,0,TRUE',\n",
       " u'39086,47614,1,?,1,?,1,1,1,1,1,TRUE',\n",
       " u'70031,70237,1,?,1,?,1,1,1,1,1,TRUE',\n",
       " u'84795,97439,1,?,1,?,1,1,1,1,1,TRUE',\n",
       " u'36950,42116,1,?,1,1,1,1,1,1,1,TRUE',\n",
       " u'42413,48491,1,?,1,?,1,1,1,1,1,TRUE',\n",
       " u'25965,64753,1,?,1,?,1,1,1,1,1,TRUE',\n",
       " u'49451,90407,1,?,1,?,1,1,1,1,0,TRUE',\n",
       " u'39932,40902,1,?,1,?,1,1,1,1,1,TRUE',\n",
       " u'46626,47940,1,?,1,?,1,1,1,1,1,TRUE']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vals.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Each time we access vals, it is *reconstructed* from the original sources\n",
    "\n",
    "Spark maintains a DAG of how each RDD was constructed so that data sets can be reconstructed - hence *resilient distributed datasets*. However, this is inefficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'37291,53113,0.833333333333333,?,1,?,1,1,1,1,0,TRUE'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vals is reconstructed again\n",
    "vals.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spark allows us to persist RDDs that we will be re-using"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[4] at RDD at PythonRDD.scala:42"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vals.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'37291,53113,0.833333333333333,?,1,?,1,1,1,1,0,TRUE',\n",
       " u'39086,47614,1,?,1,?,1,1,1,1,1,TRUE',\n",
       " u'70031,70237,1,?,1,?,1,1,1,1,1,TRUE',\n",
       " u'84795,97439,1,?,1,?,1,1,1,1,1,TRUE',\n",
       " u'36950,42116,1,?,1,1,1,1,1,1,1,TRUE',\n",
       " u'42413,48491,1,?,1,?,1,1,1,1,1,TRUE',\n",
       " u'25965,64753,1,?,1,?,1,1,1,1,1,TRUE',\n",
       " u'49451,90407,1,?,1,?,1,1,1,1,0,TRUE',\n",
       " u'39932,40902,1,?,1,?,1,1,1,1,1,TRUE',\n",
       " u'46626,47940,1,?,1,?,1,1,1,1,1,TRUE']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now vals is no longer reconstructed but retrieved from memory\n",
    "vals.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'37291,53113,0.833333333333333,?,1,?,1,1,1,1,0,TRUE',\n",
       " u'39086,47614,1,?,1,?,1,1,1,1,1,TRUE',\n",
       " u'70031,70237,1,?,1,?,1,1,1,1,1,TRUE',\n",
       " u'84795,97439,1,?,1,?,1,1,1,1,1,TRUE',\n",
       " u'36950,42116,1,?,1,1,1,1,1,1,1,TRUE',\n",
       " u'42413,48491,1,?,1,?,1,1,1,1,1,TRUE',\n",
       " u'25965,64753,1,?,1,?,1,1,1,1,1,TRUE',\n",
       " u'49451,90407,1,?,1,?,1,1,1,1,0,TRUE',\n",
       " u'39932,40902,1,?,1,?,1,1,1,1,1,TRUE',\n",
       " u'46626,47940,1,?,1,?,1,1,1,1,1,TRUE']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vals.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parse lines and work on them "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse(line):\n",
    "    pieces = line.strip().split(',')\n",
    "    id1, id2 = map(int, pieces[:2])\n",
    "    scores = [np.nan if p=='?' else float(p) for p in pieces[2:11]]\n",
    "    matched = True if pieces[11] == 'TRUE' else False\n",
    "    return [id1, id2, scores, matched]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mds = vals.map(lambda x: parse(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[10] at RDD at PythonRDD.scala:42"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mds.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "match_counts = mds.map(lambda x: x[-1]).countByValue()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False 5728201\n",
      "True 20931\n"
     ]
    }
   ],
   "source": [
    "for cls in match_counts:\n",
    "    print cls, match_counts[cls]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Summary statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(count: 5749132, mean: nan, stdev: nan, max: nan, min: nan)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mds.map(lambda x: x[2][0]).stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(count: 5748125, mean: 0.712902470443, stdev: 0.3887583258, max: 1.0, min: 0.0)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mds.filter(lambda x: np.isfinite(x[2][0])).map(lambda x: x[2][0]).stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Takes too long on laptop - skip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "stats = [mds.filter(lambda x: np.isfinite(x[2][i])).map(lambda x: x[2][i]).stats()\n",
    "         for i in range(3)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "for stat in stats:\n",
    "    print stat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the MLlib for Regression\n",
    "\n",
    "Adapted from [example](https://spark.apache.org/examples.html) in Spark doucmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 2, 4, 5, 6, 7, 8]\n",
      "5160175 574313\n",
      "5734488 5160175 574313\n",
      "Training Error = 0.00356774093569\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.classification import LogisticRegressionWithSGD\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "\n",
    "def parsePoint(md):\n",
    "    return LabeledPoint(md[-1], md[2])\n",
    "\n",
    "full_count = mds.count()\n",
    "\n",
    "# Only use columns with less than 20% missing as features\n",
    "idxs = [i for i in range(9) if \n",
    "        mds.filter(lambda p: np.isfinite(p[2][i])).count() > 0.8*full_count]\n",
    "\n",
    "data = mds.filter(lambda p: np.all(np.isfinite(np.array(p[2])[idxs]))).map(lambda p: parsePoint(p))\n",
    "train_data, predict_data = data.randomSplit([0.9, 0.1])\n",
    "\n",
    "model = LogisticRegressionWithSGD.train(train_data)\n",
    "\n",
    "labelsAndPreds = predict_data.map(lambda p: (p.label, model.predict(p.features)))\n",
    "trainErr = labelsAndPreds.filter(lambda (v, p): v != p).count() / float(predict_data.count())\n",
    "\n",
    "print \"Training Error = \" + str(trainErr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "References\n",
    "----\n",
    "\n",
    "- [Spark documetnation](https://spark.apache.org/documentation.html)\n",
    "- [Spark examples](https://spark.apache.org/examples.html)\n",
    "- [Learning Spark](http://www.amazon.com/Learning-Spark-Lightning-Fast-Data-Analysis/dp/1449358624/ref=sr_1_3?s=books&ie=UTF8&qid=1428533070&sr=1-3&keywords=spark)\n",
    "- [Advanced Analytics with Spark](http://www.amazon.com/Advanced-Analytics-Spark-Patterns-Learning/dp/1491912766/ref=sr_1_4?s=books&ie=UTF8&qid=1428533070&sr=1-4&keywords=spark)\n",
    "- [Data Algorithms](http://www.amazon.com/Data-Algorithms-Recipes-Scaling-Hadoop/dp/1491906189/ref=pd_sim_b_6?ie=UTF8&refRID=04ADYDTN1VCVADVB7HY6)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
