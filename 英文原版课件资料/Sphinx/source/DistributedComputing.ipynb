{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distributed computing for Big Data\n",
    "====\n",
    "\n",
    "- many (cheapish) machines (known as nodes)\n",
    "- loosely coupled\n",
    "- fault tolerant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why and when does distributed computing matter?\n",
    "\n",
    "- data growing exponentially\n",
    "    - Whole genome sequencing 100-200 GB per BAM file\n",
    "    - NYSE 1 TB of trade data per day\n",
    "    - Large Hadron Collider 15 PB per year (1 PB = 1,000 TB)\n",
    "- storage $\\gg$ access speeds\n",
    "    - how long does it take to read or write a 1 TB disk?\n",
    "        - parallel reads can result in large speed ups\n",
    "- most relevant for **big** data\n",
    "    - 1 billion rows\n",
    "    - $\\gg$ 128 MB (default block size for Hadoop)\n",
    "- other solutions may be more suitable\n",
    "    - shared memory parallel system \n",
    "    - Relational databases (seek time is bottleneck)\n",
    "    - Grid computing for compute-intensive jobs where netwrok bandwidth is not bottleneck (HPC, MPI)\n",
    "    - Volunteer computing (compute time $\\gg$ data transfer time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ingredients for effiicient distributed computing\n",
    "\n",
    "- Problems \n",
    "    - high likelihood of hardware failure\n",
    "    - combine data from nodes in cluster\n",
    "- Hadoop\n",
    "    - Distributed file storage (HDFS)\n",
    "    - Cluster resource mangagement (YARN)\n",
    "    - Analysis model (MapReduce, Spark, Impala)\n",
    "- Programming style\n",
    "    - Functional\n",
    "    - Declarative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Hadoop?\n",
    "\n",
    "Hadoop is a framework for distributed programming that handles failures transparently and provides a way to robuslty code programs for execution on a cluster. The main modules are\n",
    "\n",
    "- A distributed file system (HDFS - Hadoop Distributed File System)\n",
    "- A cluster manager (YARN - Yet Anther Resource Negotiator)\n",
    "- A parallel programming model for large data sets (MapReduce)\n",
    "\n",
    "There is also an ecosystem of tools with very whimsical names built upon the Hadoop framework, and this ecosystem can be bewildering. We will minly look at distributed compuitng alternatives to MapReduce that can run on HDFS - spefically `Spark` and `Impala`. Also of interest is `Mahout`, a parallel machine learing library built on top of `MapReduce` and `spark`.\n",
    "\n",
    "See the [official documnetation here](http://hadoop.apache.org/docs/current/index.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installation\n",
    "\n",
    "The simplest way to try out the Hadoop system is probbaly to install the [Cloudera Virtual Machine image](http://www.cloudera.com/content/cloudera/en/downloads/quickstart_vms/cdh-5-3-x.html) or to use [Amazon Elastic MapRedcue](http://aws.amazon.com/elasticmapreduce/). If you install [from scratch](https://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-common/SingleCluster.html), there are some confiugration steps to overcome. The following example assumes that Hadoop has been installed locally and the path to Hadoop executables has been exported."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Review of functional programming\n",
    "----\n",
    "\n",
    "```python\n",
    "lambda\n",
    "map\n",
    "filter\n",
    "reduce\n",
    "fold\n",
    "concat\n",
    "flatmap\n",
    "aggregate\n",
    "groupby\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "%precision 4\n",
    "plt.style.use('ggplot')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied (use --upgrade to upgrade): toolz in /Users/cliburn/anaconda/lib/python2.7/site-packages\r\n"
     ]
    }
   ],
   "source": [
    "! pip install toolz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from toolz import countby, groupby, accumulate, reduce, compose, partition\n",
    "from operator import add, itemgetter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anonymous functions, map and filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 4, 16, 36, 64]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = range(10)\n",
    "map(lambda x: x*x, filter(lambda x: x%2==0, x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduce, accumulate and fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reduce(add, x, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flatmap and function composition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "flatmap = compose(concat, map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['hello', 'world'], ['this', 'is', 'the', 'end']]\n",
      "['hello', 'world', 'this', 'is', 'the', 'end']\n"
     ]
    }
   ],
   "source": [
    "from string import split\n",
    "\n",
    "s = [\"hello world\", \"this is the end\"]\n",
    "print list(map(split, s))\n",
    "print list(flatmap(split, s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with key-value pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('a', 1), ('a', 1), ('b', 1), ('a', 1), ('a', 1), ('b', 1), ('c', 1), ('d', 1), ('e', 1), ('d', 1), ('a', 1)]\n"
     ]
    }
   ],
   "source": [
    "s = 'aabaabcdeda'\n",
    "a = [(_, 1) for _ in s]\n",
    "print a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 1), ('c', 1), ('b', 1), ('e', 1), ('d', 1)]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[item[0] for item in g.itervalues()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a': [('a', 1), ('a', 1), ('a', 1), ('a', 1), ('a', 1)],\n",
       " 'b': [('b', 1), ('b', 1)],\n",
       " 'c': [('c', 1)],\n",
       " 'd': [('d', 1), ('d', 1)],\n",
       " 'e': [('e', 1)]}"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "groupby(itemgetter(0), a, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a': 5, 'b': 2, 'c': 1, 'd': 2, 'e': 1}"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "countby(itemgetter(0), a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Hadoop MapReduce workflow\n",
    "\n",
    "A Hadoop job consists of the input file(s) on HDFS, $m$ map tasks and $n$ reduce tasks, and the output is $n$ files. The stages of one map-reduce iteration are:\n",
    "\n",
    "- mapper (written by programmer)\n",
    "- combiner (written by programmer)\n",
    "- sort and shuffle (done by Hdaoop framework)\n",
    "- reduce (written by programmer)\n",
    "\n",
    "At each such iteration, there is input read in from HDFS and given to the mapper, and output written out to HDFS by the reducer. Optimizing the MapReduce pipeline often consists of minimizing the I/O tranfers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Illustrating ideas behind MapReduce with a toy example of counting the number of each character in a string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "s = 'aabaabcdeda'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Map to create a key-value pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['a', 1],\n",
       " ['a', 1],\n",
       " ['b', 1],\n",
       " ['a', 1],\n",
       " ['a', 1],\n",
       " ['b', 1],\n",
       " ['c', 1],\n",
       " ['d', 1],\n",
       " ['e', 1],\n",
       " ['d', 1],\n",
       " ['a', 1]]"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xs = map(lambda x: [x, 1], s)\n",
    "xs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sort and shuffle (aggregate and transfer data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['a', [1, 1, 1, 1, 1]], ['b', [1, 1]], ['c', [1]], ['d', [1, 1]], ['e', [1]]]"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xs = sorted(xs)\n",
    "ys = []\n",
    "seen = []\n",
    "for x in xs:\n",
    "    if x[0] not in seen:\n",
    "        seen.append(x[0])\n",
    "        ys.append([x[0], [x[1]]])\n",
    "    else:\n",
    "        ys[-1][1].append(x[1])\n",
    "ys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a 5\n",
      "b 2\n",
      "c 1\n",
      "d 2\n",
      "e 1\n"
     ]
    }
   ],
   "source": [
    "for y in ys:\n",
    "    print y[0], reduce(add, y[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Hadoop MapReduce\n",
    "\n",
    "We want to count the number of times each word occurs in a set of books. We will do this in Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Start up Hadoop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "start-dfs.sh\n",
    "start-yarn.sh\n",
    "```\n",
    "\n",
    "This will generate a lot of chatter\n",
    "```bash\n",
    "15/04/06 12:42:13 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
    "Starting namenodes on [localhost]\n",
    "localhost: starting namenode, logging to /usr/local/Cellar/hadoop/2.6.0/libexec/logs/hadoop-cliburn-namenode-lister.dulci.duhs.duke.edu.out\n",
    "localhost: starting datanode, logging to /usr/local/Cellar/hadoop/2.6.0/libexec/logs/hadoop-cliburn-datanode-lister.dulci.duhs.duke.edu.out\n",
    "Starting secondary namenodes [0.0.0.0]\n",
    "0.0.0.0: starting secondarynamenode, logging to /usr/local/Cellar/hadoop/2.6.0/libexec/logs/hadoop-cliburn-secondarynamenode-lister.dulci.duhs.duke.edu.out\n",
    "15/04/06 12:42:30 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
    "starting yarn daemons\n",
    "starting resourcemanager, logging to /usr/local/Cellar/hadoop/2.6.0/libexec/logs/yarn-cliburn-resourcemanager-lister.dulci.duhs.duke.edu.out\n",
    "localhost: starting nodemanager, logging to /usr/local/Cellar/hadoop/2.6.0/libexec/logs/yarn-cliburn-nodemanager-lister.dulci.duhs.duke.edu.out\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make user direcotry (first time only)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "hdfs dfs -mkdir /user\n",
    "hdfs dfs -mkdir /user/cliburn\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transfer files to HDFS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "hadoop dfs -copyFromLocal books /user/cliburn/books\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write a mapper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%file mapper.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "import sys\n",
    "\n",
    "def read_input(file):\n",
    "    for line in file:\n",
    "        yield line.split()\n",
    "\n",
    "def main(sep='\\t'):\n",
    "    data = read_input(sys.stdin)\n",
    "    for words in data:\n",
    "        for word in words:\n",
    "            print '%s%s%d' % (word, sep, 1)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write a reducer function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%file reducer.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "from itertools import groupby\n",
    "from operator import itemgetter\n",
    "import sys\n",
    "\n",
    "def read_mapper_output(file, sep):\n",
    "    for line in file:\n",
    "        yield line.rstrip().split(sep, 1)\n",
    "\n",
    "def main(sep='\\t'):\n",
    "    data = read_mapper_output(sys.stdin, sep=sep)\n",
    "    for word, group in groupby(data, itemgetter(0)):\n",
    "        total_count = sum(int(count) for word, count in group)\n",
    "        print '%s%s%d' % (word, sep, total_count)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make programs executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "! chmod +x maper.py\n",
    "! chmod +x reducer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feed to Hadoop streaming\n",
    "\n",
    "\n",
    "The native language for Hadoop is Java, but Hadoop stremaing allows custom prograsm in other langauges to write the mapper, combiner and reducer functions. For full set of options, see http://hadoop.apache.org/docs/current/hadoop-mapreduce-client/hadoop-mapreduce-client-core/HadoopStreaming.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "hadoop jar $HADOOP_HOME/libexec/share/hadoop/tools/lib/hadoop-*streaming*.jar \\\n",
    "-file ./mapper.py    -mapper ./mapper.py \\\n",
    "-file ./reducer.py   -reducer ./reducer.py \\\n",
    "-input /user/cliburn/books/* -output /user/cliburn/books-output\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### View results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "hdfs dfs -ls /user/cliburn/books-output\n",
    "hdfs dfs -cat /user/cliburn/books-output/part-00000\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stopping haddop\n",
    "\n",
    "./sbin/stop-yarn.sh\n",
    "./sbin/stop-dfs.sh "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using MrJob\n",
    "\n",
    "The Python module [mrjob](http://mrjob.readthedocs.org/en/latest/) removes a lot of the boilerplate and can also send jobs to Amazon's implemtation of Hadoop known as Elastic Map Reduce (EMR)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied (use --upgrade to upgrade): mrjob in /Users/cliburn/anaconda/lib/python2.7/site-packages\r\n",
      "Requirement already satisfied (use --upgrade to upgrade): boto>=2.2.0 in /Users/cliburn/anaconda/lib/python2.7/site-packages (from mrjob)\r\n",
      "Requirement already satisfied (use --upgrade to upgrade): PyYAML in /Users/cliburn/anaconda/lib/python2.7/site-packages (from mrjob)\r\n",
      "Requirement already satisfied (use --upgrade to upgrade): simplejson>=2.0.9 in /Users/cliburn/anaconda/lib/python2.7/site-packages (from mrjob)\r\n"
     ]
    }
   ],
   "source": [
    "! pip install mrjob "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing word_count.py\n"
     ]
    }
   ],
   "source": [
    "%%file word_count.py\n",
    "# From http://mrjob.readthedocs.org/en/latest/guides/quickstart.html#writing-your-first-job\n",
    "from mrjob.job import MRJob\n",
    "\n",
    "class MRWordFrequencyCount(MRJob):\n",
    "\n",
    "    def mapper(self, _, line):\n",
    "        yield \"chars\", len(line)\n",
    "        yield \"words\", len(line.split())\n",
    "        yield \"lines\", 1\n",
    "\n",
    "    def reducer(self, key, values):\n",
    "        yield key, sum(values)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MRWordFrequencyCount.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Running the job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a single Python process for debugging \n",
    "\n",
    "```bash\n",
    "python word_count.py books/*\n",
    "```\n",
    "\n",
    "To run on Hadoop cluster\n",
    "```bash\n",
    "python word_count.py -r hadoop books/*\n",
    "```\n",
    "\n",
    "To run on Amazon EMR using files on S3\n",
    "```bash\n",
    "python word_count.py -r emr s3://<path_to_books>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Java version\n",
    "\n",
    "For comparison, here is the first Java version from the [official tutorial](http://hadoop.apache.org/docs/current/hadoop-mapreduce-client/hadoop-mapreduce-client-core/MapReduceTutorial.html):\n",
    "\n",
    "```java\n",
    "import java.io.IOException;\n",
    "import java.util.StringTokenizer;\n",
    "\n",
    "import org.apache.hadoop.conf.Configuration;\n",
    "import org.apache.hadoop.fs.Path;\n",
    "import org.apache.hadoop.io.IntWritable;\n",
    "import org.apache.hadoop.io.Text;\n",
    "import org.apache.hadoop.mapreduce.Job;\n",
    "import org.apache.hadoop.mapreduce.Mapper;\n",
    "import org.apache.hadoop.mapreduce.Reducer;\n",
    "import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;\n",
    "import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;\n",
    "\n",
    "public class WordCount {\n",
    "\n",
    "  public static class TokenizerMapper\n",
    "       extends Mapper<Object, Text, Text, IntWritable>{\n",
    "\n",
    "    private final static IntWritable one = new IntWritable(1);\n",
    "    private Text word = new Text();\n",
    "\n",
    "    public void map(Object key, Text value, Context context\n",
    "                    ) throws IOException, InterruptedException {\n",
    "      StringTokenizer itr = new StringTokenizer(value.toString());\n",
    "      while (itr.hasMoreTokens()) {\n",
    "        word.set(itr.nextToken());\n",
    "        context.write(word, one);\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "\n",
    "  public static class IntSumReducer\n",
    "       extends Reducer<Text,IntWritable,Text,IntWritable> {\n",
    "    private IntWritable result = new IntWritable();\n",
    "\n",
    "    public void reduce(Text key, Iterable<IntWritable> values,\n",
    "                       Context context\n",
    "                       ) throws IOException, InterruptedException {\n",
    "      int sum = 0;\n",
    "      for (IntWritable val : values) {\n",
    "        sum += val.get();\n",
    "      }\n",
    "      result.set(sum);\n",
    "      context.write(key, result);\n",
    "    }\n",
    "  }\n",
    "\n",
    "  public static void main(String[] args) throws Exception {\n",
    "    Configuration conf = new Configuration();\n",
    "    Job job = Job.getInstance(conf, \"word count\");\n",
    "    job.setJarByClass(WordCount.class);\n",
    "    job.setMapperClass(TokenizerMapper.class);\n",
    "    job.setCombinerClass(IntSumReducer.class);\n",
    "    job.setReducerClass(IntSumReducer.class);\n",
    "    job.setOutputKeyClass(Text.class);\n",
    "    job.setOutputValueClass(IntWritable.class);\n",
    "    FileInputFormat.addInputPath(job, new Path(args[0]));\n",
    "    FileOutputFormat.setOutputPath(job, new Path(args[1]));\n",
    "    System.exit(job.waitForCompletion(true) ? 0 : 1);\n",
    "  }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hadoop MapReduce Patterns\n",
    "\n",
    "Most Hadoop work flows are organized as several rounds of map/reduce - this is known as job chaining. Because I/O is so expensive, chain folding where jobs are rearranged to minimize inputs/outputs and job merging where unrelated jobs using the same dataset are run togtether are common. In `mrjob`, job chaining is performed via the [steps abstraction](http://mrjob.readthedocs.org/en/latest/guides/writing-mrjobs.html).\n",
    "\n",
    "There are several common patterns that are repeatedly used in Hadoop MapReduce programs:\n",
    "\n",
    "- Summarization (e.g. sum, mean, counting)\n",
    "- Filtering (e.g. subsampling, removing poor quality items, top 10 lists)\n",
    "- Data organization (e.g. converting to hiearhical format, binning)\n",
    "- Joins\n",
    "\n",
    "While it is certinly possible, it will take a lot of work to code, debug and optimize any non-trivial program using just MapReduce construct, for example regularized logistic regression on a large data set. Hence, we will switch our focus to more modern tools such as `Spark` and `Impala` that provide higher level abstractions and often greater efficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark\n",
    "----\n",
    "\n",
    "Spark provides a much richer set of programming constructs and libraries that greatly simplify concurrent programming. In addition, because Spark data can be persistent over a session (unliike MapReduce which reads/writes data at each step in the job chain), it can be much faster for iteratvie programs and also enables interactive concurrent programming. See [official documenttion](https://spark.apache.org/docs/latest/index.html) for details, including [setting up on Amazon](https://spark.apache.org/docs/latest/ec2-scripts.html). This [article](http://aws.amazon.com/articles/Elastic-MapReduce/4926593393724923) on how to set up Spark on EMR may also be helpful.\n",
    "\n",
    "Very conceniently for learning, Spark provides an REPL shell where you can interactively type and run Spark programs. For example, this will open a Spark shell as an IPython Notebook (if spark is installed and pyspark is on your path):\n",
    "\n",
    "```bash\n",
    "IPYTHON_OPTS=\"notebook\" pyspark\n",
    "```\n",
    "\n",
    "To whet your appetite, here is the stadnalone Spark version for the word count program."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing spark_count.py\n"
     ]
    }
   ],
   "source": [
    "%%file spark_count.py\n",
    "\n",
    "from pyspark import SparkConf, SparkContext\n",
    "conf = SparkConf().setMaster(\"local\").setAppName(\"Word Count\")\n",
    "sc = SparkContext(conf = conf)\n",
    "\n",
    "rdd = sc.textFile(\"<path_to_books>\")\n",
    "words = rdd.flatMap(lambda x: x.split())\n",
    "result = words.countByValue()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And this is run by typing on the command line \n",
    "\n",
    "```bash\n",
    "bin/spark-submit spark_count.py\n",
    "```\n",
    "\n",
    "Of course, `spark-submit` has many options that can be provided to configure the job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
