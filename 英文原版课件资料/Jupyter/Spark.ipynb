{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark on a local mahcine using 4 nodes\n",
    "====\n",
    "\n",
    "Started with\n",
    "```bash\n",
    "EXECUTOR_MEMORY=512m MASTER=local[4] pyspark\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have a Spark cluster, just set\n",
    "```bash\n",
    "MASTER=spark://IP:PORT\n",
    "```\n",
    "Everything else works the same way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check that the SparkContext object is available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.context.SparkContext at 0x10fa9a5d0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1\n",
    "\n",
    "Adapted from scala version in Chapter 2: Introduction to Data Analysis with Scala and Spark of Advanced Analytics with Spark (O'Reilly 2015)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "10 archives were successfully processed.\r\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "if not os.path.exists('documentation'):\n",
    "    ! curl -o documentation https://archive.ics.uci.edu/ml/machine-learning-databases/00210/documentation\n",
    "if not os.path.exists('donation.zip'):\n",
    "    ! curl -o donation.zip https://archive.ics.uci.edu/ml/machine-learning-databases/00210/donation.zip\n",
    "! unzip -n -q donation.zip\n",
    "! unzip -n -q 'block_*.zip'\n",
    "if not os.path.exists('linkage'):\n",
    "    ! mkdir linkage\n",
    "! mv block_*.csv linkage\n",
    "! rm block_*.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EMR.pem          Spark.ipynb      donation.zip     \u001b[31mword_count.py\u001b[m\u001b[m*\r\n",
      "EMR2.pem         \u001b[34mbooks\u001b[m\u001b[m/           frequencies.csv\r\n",
      "MapReduce.ipynb  \u001b[31mdocumentation\u001b[m\u001b[m*   \u001b[34mlinkage\u001b[m\u001b[m/\r\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Info about the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Title: Record Linkage Comparison Patterns \r\n",
      "\r\n",
      "2. Source Information\r\n",
      "   -- Underlying records: Epidemiologisches Krebsregister NRW\r\n",
      "      (http://www.krebsregister.nrw.de)\r\n",
      "   -- Creation of comparison patterns and gold standard classification:\r\n",
      "      Institute for Medical Biostatistics, Epidemiology and Informatics (IMBEI),\r\n",
      "      University Medical Center of Johannes Gutenberg University, Mainz, Germany\r\n",
      "      (http://www.imbei.uni-mainz.de) \r\n",
      "   -- Donor: Murat Sariyar, Andreas Borg (IMBEI)    \r\n",
      "   -- Date: September 2008\r\n",
      " \r\n",
      "3. Past Usage:\r\n",
      "    1. Irene Schmidtmann, Gael Hammer, Murat Sariyar, Aslihan Gerhold-Ay:\r\n",
      "       Evaluation des Krebsregisters NRW Schwerpunkt Record Linkage. Technical\r\n",
      "       Report, IMBEI 2009. \r\n",
      "       http://www.krebsregister.nrw.de/fileadmin/user_upload/dokumente/Evaluation/EKR_NRW_Evaluation_Abschlussbericht_2009-06-11.pdf\r\n",
      "       -- Describes the external evaluation of the registry's record linkage\r\n",
      "          procedures.\r\n",
      "       -- The comparison patterns in this data set were created in course of\r\n",
      "          this evaluation.\r\n",
      "           \r\n",
      "    2. Murat Sariyar, Andreas Borg, Klaus Pommerening: \r\n",
      "       Controlling false match rates in record linkage using extreme value theory.\r\n",
      "       Journal of Biomedical Informatics, 2011 (in press). \r\n",
      "       -- Predicted attribute: matching status (boolean).\r\n",
      "       -- Results:\r\n",
      "          -- A new approach for estimating the false match rate in record \r\n",
      "             linkage by methods of Extreme Value Theory (EVT).\r\n",
      "          -- The model eliminates the need for labelled training data while\r\n",
      "             achieving only slighter lower accuracy compared to a procedure\r\n",
      "             that has knowledge about the matching status.\r\n",
      "\r\n",
      "4. Relevant Information:\r\n",
      "\r\n",
      "  The records represent individual data including first and \r\n",
      "  family name, sex, date of birth and postal code, which were collected through \r\n",
      "  iterative insertions in the course of several years. The comparison\r\n",
      "  patterns in this data set are based on a sample of 100.000 records dating\r",
      "\r\n",
      "  from 2005 to 2008. Data pairs were classified as \"match\" or \"non-match\" during \r\n",
      "  an extensive manual review where several documentarists were involved. \r\n",
      "  The resulting classification formed the basis for assessing the quality of the \r\n",
      "  registryâ€™s own record linkage procedure.\r\n",
      "  \r\n",
      "  In order to limit the amount of patterns a blocking procedure was applied,\r\n",
      "  which selects only record pairs that meet specific agreement conditions. The\r\n",
      "  results of the following six blocking iterations were merged together:\r\n",
      "  \r\n",
      "    1. Phonetic equality of first name and family name, equality of date of birth.\r\n",
      "    2. Phonetic equality of first name, equality of day of birth.\r\n",
      "    3. Phonetic equality of first name, equality of month of birth.\r\n",
      "    4. Phonetic equality of first name, equality of year of birth.\r\n",
      "    5. Equality of complete date of birth.\r\n",
      "    6. Phonetic equality of family name, equality of sex.\r\n",
      "    \r\n",
      "  This procedure resulted in 5.749.132 record pairs, of which 20.931 are matches.\r\n",
      "  \r\n",
      "  The data set is split into 10 blocks of (approximately) equal size and ratio\r\n",
      "  of matches to non-matches.\r\n",
      "\r\n",
      "  The separate file frequencies.csv contains for every predictive attribute \r\n",
      "  the average number of values in the underlying records. These values can, for example,\r\n",
      "  be used as u-probabilities in weight-based record linkage following the\r\n",
      "  framework of Fellegi and Sunter.\r\n",
      "   \r\n",
      "\r\n",
      "5. Number of Instances: 5.749.132\r\n",
      "\r\n",
      "6. Number of Attributes: 12 (9 predictive attributes, 2 non-predictive, \r\n",
      "                             1 goal field)\r\n",
      "\r\n",
      "7. Attribute Information:\r\n",
      "   1. id_1: Internal identifier of first record.\r\n",
      "   2. id_2: Internal identifier of second record.\r\n",
      "   3. cmp_fname_c1: agreement of first name, first component\r\n",
      "   4. cmp_fname_c2: agreement of first name, second component\r\n",
      "   5. cmp_lname_c1: agreement of family name, first component\r\n",
      "   6. cmp_lname_c2: agreement of family name, second component\r\n",
      "   7. cmp_sex: agreement sex\r\n",
      "   8. cmp_bd: agreement of date of birth, day component\r\n",
      "   9. cmp_bm: agreement of date of birth, month component\r\n",
      "   10. cmp_by: agreement of date of birth, year component\r\n",
      "   11. cmp_plz: agreement of postal code\r\n",
      "   12. is_match: matching status (TRUE for matches, FALSE for non-matches)\r\n",
      "\r\n",
      "8. Missing Attribute Values:  \r\n",
      "\r\n",
      "  cmp_fname_c1: 1007\r\n",
      "  cmp_fname_c2: 5645434\r\n",
      "  cmp_lname_c1: 0\r\n",
      "  cmp_lname_c2: 5746668\r\n",
      "  cmp_sex:      0\r\n",
      "  cmp_bd:       795\r\n",
      "  cmp_bm:       795\r\n",
      "  cmp_by:       795\r\n",
      "  cmp_plz:      12843\r\n",
      "\r\n",
      "\r\n",
      "9. Class Distribution: 20.931 matches, 5728201 non-matches\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "! cat documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If we are running Spark on Hadoop, we need to transfer files to HDFS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "! hadoop fs -rm -rf linkage\n",
    "! hadoop fs -put block_*.csv linkage\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rdd = sc.textFile('linkage')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Actions trigger execution and return a non-RDD result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'\"id_1\",\"id_2\",\"cmp_fname_c1\",\"cmp_fname_c2\",\"cmp_lname_c1\",\"cmp_lname_c2\",\"cmp_sex\",\"cmp_bd\",\"cmp_bm\",\"cmp_by\",\"cmp_plz\",\"is_match\"'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'\"id_1\",\"id_2\",\"cmp_fname_c1\",\"cmp_fname_c2\",\"cmp_lname_c1\",\"cmp_lname_c2\",\"cmp_sex\",\"cmp_bd\",\"cmp_bm\",\"cmp_by\",\"cmp_plz\",\"is_match\"',\n",
       " u'37291,53113,0.833333333333333,?,1,?,1,1,1,1,0,TRUE',\n",
       " u'39086,47614,1,?,1,?,1,1,1,1,1,TRUE',\n",
       " u'70031,70237,1,?,1,?,1,1,1,1,1,TRUE',\n",
       " u'84795,97439,1,?,1,?,1,1,1,1,1,TRUE',\n",
       " u'36950,42116,1,?,1,1,1,1,1,1,1,TRUE',\n",
       " u'42413,48491,1,?,1,?,1,1,1,1,1,TRUE',\n",
       " u'25965,64753,1,?,1,?,1,1,1,1,1,TRUE',\n",
       " u'49451,90407,1,?,1,?,1,1,1,1,0,TRUE',\n",
       " u'39932,40902,1,?,1,?,1,1,1,1,1,TRUE']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def is_header(line):\n",
    "    return \"id_1\" in line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transforms return an RDD and are lazy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[4] at RDD at PythonRDD.scala:42"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vals = rdd.filter(lambda x: not is_header(x))\n",
    "vals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now it is evaluated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'37291,53113,0.833333333333333,?,1,?,1,1,1,1,0,TRUE',\n",
       " u'39086,47614,1,?,1,?,1,1,1,1,1,TRUE',\n",
       " u'70031,70237,1,?,1,?,1,1,1,1,1,TRUE',\n",
       " u'84795,97439,1,?,1,?,1,1,1,1,1,TRUE',\n",
       " u'36950,42116,1,?,1,1,1,1,1,1,1,TRUE',\n",
       " u'42413,48491,1,?,1,?,1,1,1,1,1,TRUE',\n",
       " u'25965,64753,1,?,1,?,1,1,1,1,1,TRUE',\n",
       " u'49451,90407,1,?,1,?,1,1,1,1,0,TRUE',\n",
       " u'39932,40902,1,?,1,?,1,1,1,1,1,TRUE',\n",
       " u'46626,47940,1,?,1,?,1,1,1,1,1,TRUE']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vals.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Each time we access vals, it is *reconstructed* from the original sources\n",
    "\n",
    "Spark maintains a DAG of how each RDD was constructed so that data sets can be reconstructed - hence *resilient distributed datasets*. However, this is inefficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'37291,53113,0.833333333333333,?,1,?,1,1,1,1,0,TRUE'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vals is reconstructed again\n",
    "vals.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spark allows us to persist RDDs that we will be re-using"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[4] at RDD at PythonRDD.scala:42"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyspark\n",
    "\n",
    "vals.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'37291,53113,0.833333333333333,?,1,?,1,1,1,1,0,TRUE',\n",
       " u'39086,47614,1,?,1,?,1,1,1,1,1,TRUE',\n",
       " u'70031,70237,1,?,1,?,1,1,1,1,1,TRUE',\n",
       " u'84795,97439,1,?,1,?,1,1,1,1,1,TRUE',\n",
       " u'36950,42116,1,?,1,1,1,1,1,1,1,TRUE',\n",
       " u'42413,48491,1,?,1,?,1,1,1,1,1,TRUE',\n",
       " u'25965,64753,1,?,1,?,1,1,1,1,1,TRUE',\n",
       " u'49451,90407,1,?,1,?,1,1,1,1,0,TRUE',\n",
       " u'39932,40902,1,?,1,?,1,1,1,1,1,TRUE',\n",
       " u'46626,47940,1,?,1,?,1,1,1,1,1,TRUE']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now vals is no longer reconstructed but retrieved from memory\n",
    "vals.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'37291,53113,0.833333333333333,?,1,?,1,1,1,1,0,TRUE',\n",
       " u'39086,47614,1,?,1,?,1,1,1,1,1,TRUE',\n",
       " u'70031,70237,1,?,1,?,1,1,1,1,1,TRUE',\n",
       " u'84795,97439,1,?,1,?,1,1,1,1,1,TRUE',\n",
       " u'36950,42116,1,?,1,1,1,1,1,1,1,TRUE',\n",
       " u'42413,48491,1,?,1,?,1,1,1,1,1,TRUE',\n",
       " u'25965,64753,1,?,1,?,1,1,1,1,1,TRUE',\n",
       " u'49451,90407,1,?,1,?,1,1,1,1,0,TRUE',\n",
       " u'39932,40902,1,?,1,?,1,1,1,1,1,TRUE',\n",
       " u'46626,47940,1,?,1,?,1,1,1,1,1,TRUE']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vals.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parse lines and work on them "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse(line):\n",
    "    pieces = line.strip().split(',')\n",
    "    id1, id2 = map(int, pieces[:2])\n",
    "    scores = [np.nan if p=='?' else float(p) for p in pieces[2:11]]\n",
    "    matched = True if pieces[11] == 'TRUE' else False\n",
    "    return [id1, id2, scores, matched]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mds = vals.map(lambda x: parse(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[9] at RDD at PythonRDD.scala:42"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mds.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "match_counts = mds.map(lambda x: x[-1]).countByValue()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for cls in match_counts:\n",
    "    print cls, match_counts[cls]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Summary statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mds.map(lambda x: x[2][0]).stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mds.filter(lambda x: np.isfinite(x[2][0])).map(lambda x: x[2][0]).stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Takes too long on laptop - skip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "stats = [mds.filter(lambda x: np.isfinite(x[2][i])).map(lambda x: x[2][i]).stats()\n",
    "         for i in range(3)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "for stat in stats:\n",
    "    print stat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.classification import LogisticRegressionWithSGD\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "\n",
    "def parsePoint(md):\n",
    "    return LabeledPoint(md[-1], md[2])\n",
    "\n",
    "data = mds.filter(lambda x: np.all(np.isfinite(x[2]))).map(lambda x: parsePoint(x))\n",
    "model = LogisticRegressionWithSGD.train(data)\n",
    "\n",
    "labelsAndPreds = data.map(lambda p: (p.label, model.predict(p.features)))\n",
    "trainErr = labelsAndPreds.filter(lambda (v, p): v != p).count() / float(data.count())\n",
    "print(\"Training Error = \" + str(trainErr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
