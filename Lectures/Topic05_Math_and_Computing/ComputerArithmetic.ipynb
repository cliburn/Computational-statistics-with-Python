{
 "metadata": {
  "name": "",
  "signature": "sha256:dca57e018807b396ff278fc207175139d5b6d4b37eec8f2b829b0ac2921b2859"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%matplotlib inline\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "%precision 4\n",
      "import os, sys, glob\n",
      "np.random.seed(1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Computer numbers and mathematics\n",
      "\n",
      "For this course, we will only be concerned with fixed point numbers (representing integers) and floating point numbers (representing reals). Since computer represnetaions of numbers are finite, they are only approximations to the integer ring and real field of mathematics. This notebook presents a small list of things to be mindful of to avoid unexpected results.\n",
      "\n",
      "**References**\n",
      "\n",
      "* <https://docs.python.org/2/tutorial/floatingpoint.html>\n",
      "* <http://introcs.cs.princeton.edu/java/lectures/9scientific.pdf>\n",
      "* <http://www.johndcook.com/blog/2008/09/28/theoretical-explanation-for-numerical-results/>\n",
      "* <http://www.johndcook.com/blog/2008/09/26/comparing-three-methods-of-computing-standard-deviation/>\n",
      "* <http://www.johndcook.com/blog/2008/10/20/comparing-two-ways-to-fit-a-line-to-data/>\n",
      "* <http://www.johndcook.com/blog/2008/11/05/how-to-calculate-pearson-correlation-accurately/>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Some examples of numbers behaving badly\n",
      "----"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Normalizing weights\n",
      "\n",
      "Given a set of weights, we want to nromalize them so that the sum = 1."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def normalize(ws):\n",
      "    \"\"\"Returns normalized set of weights that sum to 1.\"\"\"\n",
      "    s = sum(ws)\n",
      "    return [w/s for w in ws]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ws = [1,2,3,4,5]\n",
      "normalize(ws)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 3,
       "text": [
        "[0, 0, 0, 0, 0]"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Comparing likleihoods\n",
      "\n",
      "Assuming indepdnece, the likelihood of observing some data points given a distributional model for each data point is the product of the likelihood for each data point."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from scipy.stats import norm\n",
      "\n",
      "rv1 = norm(0, 1)\n",
      "rv2 = norm(0, 3)\n",
      "\n",
      "xs = np.random.normal(0, 3, 1000)\n",
      "likelihood1 = np.prod(rv1.pdf(xs))\n",
      "likelihood2 = np.prod(rv2.pdf(xs))\n",
      "likelihood2 > likelihood1"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 4,
       "text": [
        "False"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Equality comparisons\n",
      "\n",
      "We use an equality condition to exit some loop."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "s = 0.0\n",
      "\n",
      "for i in range(1000):\n",
      "    s += 1.0/10.0\n",
      "    if s == 1.0:\n",
      "        break\n",
      "print i"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "999\n"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Calculating variance\n",
      "\n",
      "$$\n",
      "s^2 = \\frac{\\sum_{i=1}^{n} x_i^2 - (\\sum_{i=1}^n x_i)^2/n}{n-1}\n",
      "$$"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def var(xs):\n",
      "    \"\"\"Returns variance of sample data.\"\"\"\n",
      "    n = 0\n",
      "    s = 0\n",
      "    ss = 0\n",
      "    \n",
      "    for x in xs:\n",
      "        n +=1\n",
      "        s += x\n",
      "        ss += x*x\n",
      "\n",
      "    v = (ss - (s*s)/n)/(n-1)\n",
      "    return v"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# What is the sample variance for numbers from a normal distribution with variance 1?\n",
      "np.random.seed(4)\n",
      "xs = np.random.normal(1e9, 1, 1000)\n",
      "var(xs)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 7,
       "text": [
        "-262.4064"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Finite representation of numbers\n",
      "\n",
      "For integers, there is a maximum and minimum representatble number for langauges. Python integers are acutally objects, so they intelligently switch to arbitrary precision numbers when you go beyond these limits, but this is not true for most other languages including C and R. With 64 bit representation, the maximumm is 2^63 - 1 and the minimum is -2^63 - 1."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import sys\n",
      "sys.maxint"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 8,
       "text": [
        "9223372036854775807"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "2**63-1 == sys.maxint"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 9,
       "text": [
        "True"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Python handles \"overflow\" of integers gracefully by \n",
      "# swithing from integers to \"long\" abritrary precsion numbers\n",
      "sys.maxint + 1"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 10,
       "text": [
        "9223372036854775808L"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Integer division\n",
      "\n",
      "This has been illustrated more than once, becuase it is such a common source of bugs. Be very careful when dividing one integer by another. Here are some common workarounds."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Explicit float conversion\n",
      "\n",
      "print float(1)/3"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.333333333333\n"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Implicit float conversion\n",
      "\n",
      "print (0.0 + 1)/3\n",
      "print (1.0 * 1)/3"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.333333333333\n",
        "0.333333333333\n"
       ]
      }
     ],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Telling Python to ALWAYS do floaitng point with '/'\n",
      "# Integer division can still be done with '//'\n",
      "# The __futre__ package contains routines that are only \n",
      "# found beyond some Python release number.\n",
      "\n",
      "from __future__ import division\n",
      "\n",
      "print (1/3)\n",
      "print (1//3)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.333333333333\n",
        "0\n"
       ]
      }
     ],
     "prompt_number": 13
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "[Documentation about the fuure package](https://docs.python.org/2/library/__future__.html)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Overflow in langauges such as C \"wraps around\" and gives negative numbers"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%file demo.c\n",
      "\n",
      "#include \"limits.h\"\n",
      "\n",
      "long limit() {\n",
      "    return LONG_MAX;\n",
      "}\n",
      "\n",
      "long overflow() {\n",
      "    long x = LONG_MAX;\n",
      "    return x+1;\n",
      "}"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Overwriting demo.c\n"
       ]
      }
     ],
     "prompt_number": 14
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "! clang -emit-llvm -c demo.c -o demo.o"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 15
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import bitey\n",
      "import demo\n",
      "\n",
      "demo.limit(), demo.overflow()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 16,
       "text": [
        "(9223372036854775807, -9223372036854775808)"
       ]
      }
     ],
     "prompt_number": 16
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Floating point numbers\n",
      "A floating point number is stored in 3 pieces (sign bit, exponent, mantissa) so that every float is represetned as get +/- mantissa ^ exponent. Because of this, the interval between consecutive numbers is smallest (high precison) for numebrs close to 0 and largest for numbers close to the lower and upper bounds."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from IPython.display import Image"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 17
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "Image(url='http://www.dspguide.com/graphics/F_4_2.gif')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<img src=\"http://www.dspguide.com/graphics/F_4_2.gif\"/>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 18,
       "text": [
        "<IPython.core.display.Image at 0x112996810>"
       ]
      }
     ],
     "prompt_number": 18
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "Image(url='http://jasss.soc.surrey.ac.uk/9/4/4/fig1.jpg')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<img src=\"http://jasss.soc.surrey.ac.uk/9/4/4/fig1.jpg\"/>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 19,
       "text": [
        "<IPython.core.display.Image at 0x10ec5c8d0>"
       ]
      }
     ],
     "prompt_number": 19
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Information about the floating point reresentation on your system can be obtained from `sys.float_info`. Definitions of the stored values are given at <https://docs.python.org/2/library/sys.html#sys.float_info>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import sys\n",
      "\n",
      "print sys.float_info"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "sys.float_info(max=1.7976931348623157e+308, max_exp=1024, max_10_exp=308, min=2.2250738585072014e-308, min_exp=-1021, min_10_exp=-307, dig=15, mant_dig=53, epsilon=2.220446049250313e-16, radix=2, rounds=1)\n"
       ]
      }
     ],
     "prompt_number": 20
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Floating point numbers may not be precise\n",
      "'%.20f' % (0.1 * 0.1 * 100)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 21,
       "text": [
        "'1.00000000000000022204'"
       ]
      }
     ],
     "prompt_number": 21
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Because of this, don't chek for equality of floating point numbers!\n",
      "\n",
      "# Bad\n",
      "s = 0.0\n",
      "\n",
      "for i in range(1000):\n",
      "    s += 1.0/10.0\n",
      "    if s == 1.0:\n",
      "        break\n",
      "print i\n",
      "\n",
      "# OK\n",
      "\n",
      "TOL = 1e-9\n",
      "s = 0.0\n",
      "\n",
      "for i in range(1000):\n",
      "    s += 1.0/10.0\n",
      "    if abs(s - 1.0) < TOL:\n",
      "        break\n",
      "print i"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "999\n",
        "9\n"
       ]
      }
     ],
     "prompt_number": 22
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Loss of precision\n",
      "1 + 6.022e23 - 6.022e23"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 23,
       "text": [
        "0.0000"
       ]
      }
     ],
     "prompt_number": 23
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Addition may not be associative\n",
      "6.022e23 - 6.022e23 + 1"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 24,
       "text": [
        "1.0000"
       ]
      }
     ],
     "prompt_number": 24
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<font color=blue>Lesson: Avoid algorithms that subtract two numbers that are very close to one anotoer. The loss of significnance is greater when both numbers are very large due to the limited number of precsion bits available.</font>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# loss of precision can be a problem when calculating likelihoods\n",
      "probs = np.random.random(1000)\n",
      "np.prod(probs)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 25,
       "text": [
        "0.0000"
       ]
      }
     ],
     "prompt_number": 25
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# when multiplying lots of small numbers, work in log space\n",
      "np.sum(np.log(probs))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 26,
       "text": [
        "-980.0558"
       ]
      }
     ],
     "prompt_number": 26
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<font color=blue>Lesson: Work in log space for very small or very big numbers to reduce underflow/overflow</font>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Using arbitrary precision libraries\n",
      "----\n",
      "\n",
      "If you need precision more than speed (e.g. your code is likely to underflow or overflow otherwise and you cannot find or don't want to use a workaround), Python has support for arbitrary precison mathematics via \n",
      "\n",
      "- [The mpmath package](http://mpmath.org)\n",
      "- [The gmpy2 package](https://pypi.python.org/pypi/gmpy2)\n",
      "\n",
      "both of which can be installed via pip\n",
      "```bash\n",
      "pip install gmpy2\n",
      "pip install mpmath\n",
      "```\n",
      "\n",
      "These packages allow you to set the precsion of numbers used in calculations. Refer to the documentation if you need to use these libraries."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Stability and conditioning\n",
      "\n",
      "Suppose we have a computer algorithm $g(x)$ that represents the mathematical function $f(x)$. $g(x)$ is stable if for some small perturbation $\\epsilon$, $g(x+\\epsilon) \\approx f(x)$\n",
      "\n",
      "That is, the <font color=red>algorithm</font>$g(x)$ is **numerically stable** if it gives *nearly the right answer to nearly the right problem*. For example, the Newton-Raphson method can be numerically unstable for particular initial conditions and diverge even if there is a nearby root.\n",
      "\n",
      "A mathematical function $f(x)$ is well-conditioned if $f(x + \\epsilon) \\approx f(x)$ for all small perturbations $\\epsilon$.\n",
      "\n",
      "That is, the <font color=red>function</font>$f(x)$ is **well-conditioned** if the *solution varies gradually as problem varies*. For example, inverting a nearly singluar matrix is a poorly condiitioned problem."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Unstable version**"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Catastrophic cancellation occurs when subtracitng \n",
      "# two numbers that are very close to one another\n",
      "# Here is another example\n",
      "\n",
      "def f(x):\n",
      "    return (1 - np.cos(x))/(x*x)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 27
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "x = np.linspace(-4e-8, 4e-8, 100)\n",
      "plt.plot(x,f(x));\n",
      "plt.axvline(1.1e-8, color='red')\n",
      "plt.xlim([-4e-8, 4e-8]);"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAXIAAAENCAYAAAASUO4dAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzt3XmYFNW5BvD3Y3MDRCGgAYKoSFxwBxFzwyhKRq/CjY+J\nQY1R45XrEjUal+g1wvW6JUZRMQjGLUFFoyRgIihRR00CeE1kC7sCsqvIFllk4Nw/vi6n6KnqOtVd\np6qbfn/PwwPT091z6Ol+++tT3zklxhgQEVHlapL1AIiIqDQMciKiCscgJyKqcAxyIqIKxyAnIqpw\nDHIiogoXGeQi8oSIrBaRmQWu85CILBCR6SJyTLJDJCKiQmwq8icB1IZ9U0TOAHCwMaYbgMsAjEho\nbEREZCEyyI0x7wBYW+AqAwA8nbvuVABtRKRDMsMjIqIoScyRdwSw1Pf1MgCdErhfIiKykNTBTsn7\nmuv+iYhS0iyB+1gOoLPv6065y3YiIgx3IqIiGGPyi+WdJFGRjwdwIQCISG8A64wxq0MGU/Z/br/9\n9szHwHFyjFbjLNPXVMU+nmX6x0ZkRS4izwHoC6CdiCwFcDuA5rlgHmmMeUVEzhCRhQA+B3Cx1U8m\nIqJERAa5MWaQxXWuSmY4REQUF1d25qmpqcl6CFY4zuRUwhgBjjNplTJOG2I7B1PyDxIxaf0soqog\nAvA1tcsTEZgUDnYSEVGGGORERBWOQU5EVOEY5EREFY5BTkRU4RjkREQVjkFORFThGORERBWOQU6J\nGTQIWLIk61FUh6eeynoEVE4Y5JSY114DPv4461FUhw8+yHoEVE4Y5JSIf/0L+OwzYNu2rEdSHfg4\nkx+DnBLhTanU12c7jmrBx5n8GOSUCAZ5urzHefv2bMdB5YFBTolYvFj/ZpCnw3ucV67MdhxUHhjk\nlAhW5OnyHmd2CRHAIKeEMMjTxSAnPwY5JWLJEqB9e3ZTpMV7nBnkBDDIKSFLlgAHHcSKPC2syMmP\nQU4l27IFWLMG+NrXGORp8R5n7yAzVTcGOZVs6VKgY0dgt90Y5GlhRU5+DHIq2ZIlwAEHAM2aMcjT\n4j3OH33E8y8Tg5wSsGQJ0KULgzxN3uO8++7Ap59mOxbKHoOcSrZ4cUOQs2slHd7j3KUL58mJQU4J\nYEWePu9x7tKF8+TEIKcEMMjTxyAnPwY5lcw72Nm8OYM8Ld7jfMABDHJikFOJ6uuBFSuATp1YkafJ\nX5FzjpwY5FSS5ct1aX6LFgzyNHFqhfwY5FQSb34cYJCniUFOfgxyKok3Pw6w/TBN3uO87756col1\n67IdD2WLQU4l8XrIAVbkafIeZxFW5cQgpxL5p1bYtZIe/+PMICcGOZWEc+TZYJCTH4OcSsIgzwaD\nnPwig1xEakVkrogsEJGbAr7fTkQmisg0EZklIhc5GSmVnR07dAtbBnn6/I8zFwVRwSAXkaYAhgOo\nBXAYgEEicmje1a4C8L4x5mgANQB+KSLNHIyVyszq1UCrVsCee+rX7FpJj/9x5qIgiqrIewFYaIxZ\nbIzZBmAMgIF511kJoHXu360BrDHGsC6rAosWNbQeAqzI0+R/nLt2BT78MLuxUPaigrwjgKW+r5fl\nLvN7DMDhIrICwHQA1yQ3PCpn8+cD3bs3fM2ulfT4H+f27fXrNWuyGw9lK2oKxObcI7cAmGaMqRGR\ngwBMEpGjjDEb8684ZMiQL/9dU1ODmpqaGEOlcjN/PnDIIQ1fsyJPj/9xFtHfw4IFQNu22Y2JklFX\nV4e6urpYt4kK8uUAOvu+7gytyv36ALgTAIwxH4jIIgDdAbyXf2f+IKfKN28e8N3vNnzNIE+HMbqa\n0++QQ/T30bt3NmOi5OQXuUOHDo28TdTUynsAuonIASLSAsC5AMbnXWcugFMBQEQ6QEOcM3ZVIH9q\nhUGeju3bgaZNd76se3f9fVB1KhjkuYOWVwF4FcBsAM8bY+aIyGARGZy72l0AjheR6QD+DOBGY8xn\nLgdN2du+HVi4EOjWreEydq2kY9s2faz9vIqcqlNkm6AxZgKACXmXjfT9+1MAZyU/NCpnS5cC7doB\ne+3VcBkr8nTU1+eCfGvDZazIqxtXdlJR8g90AgzytHwZ5D7duuknpB07shkTZYtBTkWZN69xkLP9\nMB319fpY+7VsCeyzj35SourDIKei5B/oBFiRpyWoIgc4vVLNGORUlKCKnEGejrAg5wHP6sUgp6Kw\nIs8OK3LKxyCn2DZvBlatatj10MP2w3QEtR8CWpEzyKsTg5xiW7hQN2rKDxNW5Ong1ArlY5BTbEHT\nKgC7VtIS1LUC6JvrypXAli3pj4myxSCn2IIOdAKsyNMSVpE3a6bbCi9cmPqQKGMMcootrCJnkKcj\nLMgBHvCsVgxyii1oVSfAIE9LoSDnAc/qxCCn2ApNrbBrxb2wrhWABzyrFYOcYlmzRnc+bN++8fdY\nkaeDUyuUj0FOsXjVuEjj77FrJR1hXSsAK/JqxSCnWMIOdAKsyNNSqCLv0EGnXnj+zurCIKdY5s3b\n+WQSfk2a6Daq3ErVrUJB7p2/k9Mr1YVBTrHMmgUccUTw90Q0YPLPJ0nJKhTkAHD44fp7ourBIKdY\npk8Hjjoq/PvsXHGvUNcKoL+f6dPTGw9lj0FO1tau1T9du4Zfh/Pk7kVV5EceCcyYkd54KHsMcrI2\nYwbQo4fOhYdh54p7hbpWAK3IZ8wAjElvTJQtBjlZi5pWAViRpyGqIvdOir1kSXpjomwxyMnajBn6\nsb0QBrl7UUEOcHql2jDIyRor8vJgE+Q84FldGORkpb4emD1b58gLYdeKe1FdKwCDvNowyMnKwoXA\nfvsBrVoVvh4rcvc4tUL5GORkxWZaBWCQp8EmyLt3B5YtAz7/PJ0xUbYY5GRlxgy7IGf7oXtR7YeA\nBv2hh3KFZ7VgkJOV6dOjO1YAVuRpsKnIAf19cZ68OjDIyQqnVsqHbZDzgGf1YJBTpM8+A9av1xP7\nRmGQuxcnyHnAszowyCmSzdJ8D9sP3bNpPwQaOle4VH/XxyCnSLbTKgAr8jTYVuRt22q76OLFzodE\nGWOQUySbpfkedq24Z9O14mE/eXVgkFMkVuTlxbYiB3jAs1owyKmgL74A5syJXprvYZC7FyfIjz4a\neP99t+Oh7EUGuYjUishcEVkgIjeFXKdGRN4XkVkiUpf4KKmgLVvcnV5txgzgoIOAli3trs8gdy9O\nkJ9wAjBlirsDnps38xyt5aBgkItIUwDDAdQCOAzAIBE5NO86bQA8AuAsY8wRAM5xNFYKcf75wJgx\nbu57yhSgd2/767NrxT3brhUA6NJFQ3zpUjdjufhi4Le/dXPfZC+qIu8FYKExZrExZhuAMQAG5l3n\nPAAvGWOWAYAx5tPkh0lhNm0CJkwAPvnEzf0XE+SsyN2KU5GL6O9vyhQ3Y/n4Y+Cll9zcN9mLCvKO\nAPzv5ctyl/l1A7CviLwpIu+JyPeTHCAVNmmSfrzdsMHN/ccNcnatuBenawVwG+QbNuhzkJtzZSsq\nyG1m1poDOBbAGQC+BeA2EelW6sDIzvjxQOfOwMaNyd/3J5/on69/3f42rMjdi1ORA26DfONG4Ctf\nAf78Zzf3T3aing7LAXT2fd0ZWpX7LQXwqTFmM4DNIvI2gKMALMi/syFDhnz575qaGtTU1MQfMX1p\n+3bgj38ELrlEl9En7d13gV697FZ0ehjk7sUN8uOP1xbErVuB3XZLdiwbNgAXXQSMGwcMzJ90paLU\n1dWhrq4u1m2ing7vAegmIgcAWAHgXACD8q4zDsDw3IHR3QCcAOD+oDvzBzmVbsoUoEMHXfQxfryb\n+48zrQIwyNMQN8hbtgS6ddMw79Ur2bFs2ABccAFw8slaWDRtmuz9V6P8Info0KGRtylYaxlj6gFc\nBeBVALMBPG+MmSMig0VkcO46cwFMBDADwFQAjxljZhf5f6gKv/1tMu2CXhXUurWbOfJig5xdK27F\n6VrxuJheqa/X1tfDDgP23x+YPLn0+1y0CHjrrdLvp9pEfmg2xkwwxnQ3xhxsjLk7d9lIY8xI33Xu\nM8YcbozpYYx5yOWAK90//wlceGEy+1+MH+8uyLdv16mVE06IdztW5O7FrcgBN0G+caPu5SKiz8Mk\nPhWOGwdceil70+Piys6UjRqlf69eXdr9zJunL6Rjj3UT5HPnAu3bA+3axbsdu1bci9u1ArgJ8g0b\n9LkHaJCPG1f6fa5ereeHffPN0u+rmjDIY1qzpvjbbt4MjB6t+1+UGuTjxgEDBuiBSBdBXsy0CsCK\nPA3FVOSHHAKsXVv6887PH+THHqstiHPnlnafq1cDxxwDjBwZfd0wmzbpn2rCII9hwwbg4IOBf/yj\nuNv/7nd6sOmEE5IJcq9LoHXr5NsPGeTlq5ggb9JEn3tTpyY3jo0bG4JcRAuLUqvy1auB66/X3vRi\nXyO/+IWudq4mDPIYRowA1q0DFjRqrLQzciQweLB2mqxaVfw4Vq4EZs/WTgFA5yk3bEh2Pw0Gefkq\nJsiB5KdX/BU5AHz728DYsaXd56pV+unh7LOBp54q7j7mzwf+8Ac9HlUtGOSWNm8GHngAOP10PbIe\n16xZeoDzzDM1yEupyF94Qatxrye4RQutuLZuLf4+/TZsAD780H4Pcj92rbhXTNcKoEGeZEW+YYMW\nEZ6TT9bn+IcfFn+fq1fr6+Oyy4DHHivuoOeiRcAZZwB33138OCoNg9zS44/rC+HMM4sL8lGjdOFO\ns2alB/lzzwGD8rr5k5wn/7//03nKuAfUAFbkaSi2Iu/VS3+3Sf1+8ivyZs2Ac87R52cxjNG9W9q3\n17G2bAm88Ub8+1m0SKdXJk4s7U2lkjDILXzxhT4xbrkFOPDA+E+OTZuAZ57RtiqgtCD/4AN9ovbr\nt/PlSQb5W28B//Zvxd2WQe5esUHeti3wta8ltz95fpADWmAUG+Rr1wJ77gnsvrvOuV92WfyDnps2\n6f18/evA5ZcDP/95cWOpNFUV5B9+WFzIPPusztv16gV07Rq/Ih8zRqv5Ll3061KCfMwYrXryX8hJ\nBvnrrzd+o7DF9kP3imk/9PTrp7/fJAQFeZ8+evnMmfHvz5tW8Zx/vu7hsmKF/X0sXqyvsyZNgGuu\n0WnIOLf3j2X9+vi3y0rFBnncA3tr1wI9e+pBkDi2b9e5tltu0a+7dNG9nW1XZhqjc+vXXNNwWSlB\nHjStAiQX5Bs36lLuPn2Kuz0rcveKrcgB4JRTipuuCBIU5E2aAN/7XnFV+apVwH77NXy9994a5o88\nYn8fixZpsQXoGogf/AD45S/jj+WGG3QPmbhcncAjSkUG+TPPAP/xH/Fu8z//owcD456I9oUX9COp\nt/XB7rvrE2T5crvb//nP+ss97bSGy1q31gNWcbf+nDlTXzxBIZtUkL/zjr7h7blncbdnkLtXSpD3\n7atL6ZM4MB4U5IAWGmPGxA+1/Ioc0AJo1Cj7vnB/kAPayvjUU/ELpxkztAUyzpve2rXanrxkSbyf\nlYSKC/JPPgGuu04/Htp2R8ybp/ub3HtvvI989fXA7bcDd9yhc3aeONMrDzwA/PjHO99eRCuPuE+u\n557TaidoN8KkesnfeEOrtmIxyN0rJcjbtAEOPTSZNkR/H7nf0UdrJ1XcDpmgIO/WTQuX3/zG7j7y\ng7xTJ93UK04Hy7Ztmhm/+pW+dm0/fU+dCnz0EXDllelX5pkH+Vtv6cY7tq67Tn8xXbvanx38hhuA\nm2/WgIoT5E8/rU+E/Pli2wOec+YAf/978OKEuNMrxmiVEzStAjT0kpfq9ddLD3K2H7pVbPuhJ6np\nlfz2Q49IcQc9g4Ic0DAdNsyuFXHRIn19+t1yixZytqe7W7BAX/ff/75O7zz5pN3tJk8Grr1Wx/Di\ni3a3AXRLgoUL7a8fJNMgnzxZn1S//73d9SdN0o/+Q4faL26YNEkXz/zoR/ruvmKF3ZTG1q06HXPn\nnY2/Z1uRP/gg8F//pdMx+eIG+dSpWuUcfXTw95OYWlmzRrtiStnqlBW5e6VU5IAWJkkFeVBFDmiQ\nv/BCvF0+w4K8b19gjz20nTBKfkUONPSl33GH3ThmzgR69NA3pAceAG67ze61NXky8M1v6lTQNdfo\n4kEbt90G1NYC//qX3fWDZBbkGzdqZV1bC7z2WvT1N23SUPzVr7S/9MQTo7fNrK/XCv6++3TxTLNm\nQPfudiu+Ro3SX+aJJzb+nk2Qf/op8PzzwBVXBH8/bpA//bQ+Xv4pGr8kgryuDvjGN4rviADYtZKG\nUrpWAOCkk4Bp00oLDqBwkB9yiJ65yua17QkLchGtyh94oPDtjdFPyvlBDuin8rFj7SpfL8gB4Ljj\ndBFgUEHnt2OH7hbau7c+vgMH6ixAlB079Dha9+76fyxWZkF+7bW6EmzYMP1lR80pDR2qleIZZ+jX\nNkH+yCP6xPCfuaRHj+jplc8/1zm1//3f4O/bBPnIkXpANuiJCcQL8s8/1zeFQkfRkwjyUtoOPazI\n3fKmF+KctSnfnntqQP3lL6WNpVCQA7pu4te/tr+//K4Vv+99TwuwQs0Ka9fq3/vs0/h7++6rVbLN\nuW38QQ5oiD/xROENwebM0VPefeUr+vXddwMvv6wzCIVMm6bNFGPG6KekuF11nkyCfOxYnRsfNkyP\n8rZooQ9EmClTtCJ98MGGy7p3L7yb2/Ll+lHqkUd2rmJtgvzhh7UyDZvG6Nq18Bz5pk16H9dfH36d\nOEH+4ot6wKdTp/DrJBHkpR7oBBjkrpU6reI55ZTS+8mjgnzQIH1O2T7PwypyQDPi6qu1YSGMN60S\n9qn12mt1qjXq9Z8f5Pvvr9Mfl18eXnBOnrzz3kRt2ujswcUXF57Kfe01oH9/PdYwerTOOqxcWXh8\nQVIP8uXLdbph9GidIhHR1rywj2CbNmkv6PDhunTX06SJ7iIYNk/+4x/rA9+9+86X9+hR+F394491\nKiasGgeAjh11Pnnz5uDvP/aYBu8RR4TfR5wgf+yxhlWhYUoN8uXLdTroqKOKvw+AQe5aUkFe6jy5\nMQ0nlgjTqpVufvX003b39/HH4UEOaG689lr4pnVB8+P547n1Vp1mCbNxo34yOPjgxj97/XptfQ4y\nZUrjadiBA3Wa5aabwn+eF+SA3n7wYA3/uHvMpBrk9fXAeedpe47/3at///Agv+UW/Rh4zjmNv9e7\nd/D0ysSJ2i3iLeLx8yrysHfWIUN0LvqQQ8L/H02b6lLnoH7RLVt0Of+tt4bfHtCPkDY7IM6Zowcg\n//3fC1+v1PbDN97QXvlSPrID7FpxrdSOFU/PnhqIxZ60e/NmrZKj5uq96ZWoqdN167QpIKgxwNO6\ntWZHWCthUMdKvssv1+uFHTidNUvbM/PPPdqsGfDoo/om4E3h+OVX5J4HH9StfYM+/Xz+ue5907dv\nw2X//d/6Or7vvsL/j3ypBvmQIXrQMT9gTzlF5+vyFym89Zbu4T18ePD9nXhi44p882b9ZQ8frke6\n8331q3okPaga/uc/dRrjZz+L/r+EzZM/9ZRWtccdV/j2thX544/r3HjUC6bU9sNS2w49rMjdSqoi\nb9FCq8WYJ2v/UljrYb7evfW5GzVXXGhaxe/qqzUYg06VGHag0695cw3J668Pfp7mT6v49eqlx73y\n82vdOu0fD9ottE0b/UR9ySWNX59vv60n5PA/js2b63z5/fcDf/1r4f+LX6pB/uST2s+Z/2637776\nLvi3vzVctmGDfsQYOVK/H+SEE7Ty9v9C7rxTd+47/fTg24iEz5P/5CdaSYf9PL+gIN+2DbjnHn1X\njWIT5F98oQshLrkk+v5KmVrZsQOYMAH41reKu70fu1bcKrVjxa+2FnjlleJuGzU/7hGxO+i5apVd\nkO+7r7YSBs2VR02teM48Uz8RP/ZY4+8VCnIAuOsuPSDpnwmYOlULt7A32NpafW1de+3Ol0+a1DCt\n4te5sz5egwbpdKeNVIN89OjwX1b+9MqVVwKnnqoPepg2bfQ/7YXyP/6hbYMPP1x4HEFBPnGiTmFc\nfnn0/wMIPuA5erTOrQW1LObbe28N6rB5dkBPZnv44dr/HqWUIJ86VY+2H3RQcbf3Y0XuVlIVOQCc\ndZZ2VsTp9fbYBjmgC2vGjy/cV716dXjHSr7rrtMurvxtMmyDXEQr3qFDG2+MFRXk++wDPPSQFlfe\nQkabk7D88pf6qeR3v2u4zD8/nu/MM4Fzz9XjgzZSDXLvjDZBTjtN36EADcT33ovuGwUa2hC/+EIr\n+Pvu06PMhRx55M5Bvm2bVuO/+IV+5LSRX5HX1+u7tU01DuiTqX37wlX5qFHAD39od3+lBPn48Tu3\naJaCQe5WkkF+4IH6HHz33fi3jRPk7dppVTp6dPh1bKdWAC06LrpIX6+eHTv0mNUBB9jdx1FHaVj6\nmxqM0VyIOqHKd76jBdbQofr15MnRxVurVrqL6pVX6jiXL9fulGOPDb/NXXfZ9/pnvkTf07u3HnyZ\nOlU7TsaMAfbaK/p2XpDfc0/Dstoo+RX5Qw/p3PmAAfbjPfDAnYP86ae1m8V/4CJKoemV2bO1uybo\nIG+QvfbS6r6Y6mr8+Hj/90IY5G4lGeSAvoGPHx//dnGCHNAAe/jh8G6MOEEOADfeqNO0H32kX69c\nqdVynM3e7rxTX7feAsGVK/Vgv804HnlEp4rffVczy+a0iD17asF43nk6A9CvX+NpZr/mzYE337T7\nv5RNkLdooSHYv79WtbZtcL176/zu8OE6nx7WQ+p3xBHaDbJ9O7BsmR4Fz+83j+KvyDdv1gO599wT\n7z4Knbuz0PL+ICL6rh+3c2XhQm2l7Nkz3u3CsGvFraS6VjwDBqQT5N/4hrYbT5gQ/P24Qb7ffvr6\n8Bb42E6r+HXooLe/4oqGatxbmm9z2/vv1/OU7rOP/dh/8hN9s7nuuvBpFT/bLrKyCXJAq8/+/fXI\ntK3DDmvYM7zQghm/Vq30I+UHH2j1f+WVdvPQfm3banW0dq2+CRx/fPyTFYftgLhmje5TYTtf7ylm\neuXll3WutNS2Qw8rcreSrsiPP15bEONu2hQ3yEX0YN+wYcHfjxvkgLYC/vGP+unVpmMlyODBulZl\n9Ojo+fF8gwbp43fSSfa3adJEGxg6dUqmucCT4FOidBdeqH/iaNJEf5G2B0o8PXroHNv779tvkekn\nok+cadP0dFJvvRX/PsKmVkaNKry8P0wxFfn48VodJIVdK24l2bUC6OvHO+gZZ6+PqMVAQb77XZ0S\nmTWr8WI5264VvzZt9P5uvVVXYRcT5E2bAiNG6GPQs2e8KUYRnQKOs3sroMfwbPZ7iqOsKvJi7b9/\nvCkNQIP817/WebugfnMbXbvq/g1nnaXtk3EFBfm2bVrh+88oZCtuRf7ZZ9rpc+qp8X9WGFbkbiVd\nkQPFTa/ErcgBXUNyxRU7b7XhidO14nflldoY8dJLxQU5oFX12Wfrm1mcihzQ7Aja2yVtu0SQF6N/\nf31ShfWb2zjwQGD+fLuNeIIEBfmLL+o0T9g+L4XEDfIJE7STqNg3siAMcrdcBHm/fvqGHmeVZzFB\nDuhUxosv6gliPDbL88PssYe+/mbOjF7VWcidd+pUR6FtNcpZ1QZ5377xzgUYpF8/fQJ07lzc7fOD\n3Du/Z/7CAVtxg3zcuOS6VTwMcrdcBPkee+gbepzFQcUGefv2Wv2OHNlw2fr12uxQbEHxgx/o7ohx\nq2m/Nm20k8SmU64cVW2QJ+HMMwvvcBglP8jfeUerokKLoAqJE+Rbt+qChKg9XOJikLvlIsgBfUMf\nN87++sUGOaCFyiOPNCyGK+ZAp1+zZno2onbtir+PSscgz1B+++Edd+hm9IV6SwuJE+QTJ+rCh1Je\nQEHYfuhW0u2HngEDdEGe7QKUUoK8Rw89sOgt2y81yIlBnql99tEj3lu26DLf+fPjd+34xQnyZ57R\nXR6TxorcLVcVebt22utte2KDUoIc0P29f/5z/WRYTMcK7YxBniH/Mn2vGrfdIiCIbfvhhg3Aq6/a\nrxqNwwvytM8iXi2Sbj/0O//88P228xXTfujXs6ceWHzqqeI7VqgBgzxjHTpo98j06bpXTClsK/Kx\nY/Xgls0uj3E1aaJ/4m6MT3ZcVeSATq9MmaIdJFFKrcgBrcrvuUdXV7MiLw2DPGMdOuiWBDfcYL8c\nP4xtkD/zjFZfrnB6xR2XQb7XXnqg/fnno6+bRJD36aMtg48/ziAvFYM8Yx066MHN//zP0u/LJshX\nrNAFFMV2xthgkLvjMsgBfYMvtEshoAdct26Nt0FVmNtu0y0pGOSliQxyEakVkbkiskBEQs8+JyI9\nRaReRM5Odoi7tj59tBc9iReFTZCPGaPL/5NcBJSPnSvuuOpa8Zx6qp59J+y8mIDOj7duHX81dZC+\nfXXH0sMOK/2+qlnBIBeRpgCGA6gFcBiAQSLSaDF67nr3ApgIIIFfb/W49NLoEyvbsgly19MqACty\nl1xX5M2a6QkNnn02/DpJTKt4RHSvo0LnyKVoURV5LwALjTGLjTHbAIwBEHQKgh8BeBHAJwHfo5RE\nBfncubrncqETfCSBG2e547JrxXPBBfqGH9Z5lGSQUzKigrwjgKW+r5flLvuSiHSEhvuI3EVsPMtI\n1AmYf/Mb3Xqz2AVHtliRu+O6Igca9qbPP7G5x/bEy5SeqCC3CeVhAG42xhjotAqnVjLi9ZEHVVJb\ntwJPPKEnrnWNQe5OGkEuos+TRx8N/r43R07lI+opsRyAf0uoztCq3O84AGNEj3y0A3C6iGwzxjTa\nGHOIb5vAmpoa1NTUxB8xhWrRQl/kW7Y0Ppg5dqyeZ7B7d/fjYJC7k0aQA7qm4aCDtKOkbdudv8ep\nFbfq6upQV1cX6zZRT4n3AHQTkQMArABwLoBB/isYY77cPFJEngTwclCIAzsHObnhzZPnB/mIEcXt\ncV4Mdq2447prxdO2rZ7P88kn9fRkfgxyt/KL3KHeWZ4LKDi1YoypB3AVgFcBzAbwvDFmjogMFpHB\nJY2WnAg64Dlzpp7WLukta8OwIncnrYoc0FMNPvpo41W6DPLyE/mUMMZMADAh77KRIdctcZE5lSoo\nyB99VBc0ZdllAAAMFklEQVQcue528LBrxZ00ulY8J5ygz6dJk3Y+vySDvPxwZecuJj/IN27UvZqT\nWDlqixW5O2lW5CJalY8YsfPlDPLywyDfxeS3ID7zjPaNd+wYfpukMcjdSTPIAeC88/SEJx991HAZ\n2w/LD4N8F9O6dcNWtsbomVguvzzdMTDI3Uk7yPfaSxcI+VsR2X5Yfhjkuxj/1Morr+jin3790h0D\nu1bcSatrxe/qq4FRoxqeV5xaKT8M8l2MP8jvvltPVpHE5kZxsCJ3J+2KHNB+8v79G6pyBnn5YZDv\nYrwgf+cdPfPKd76T/hjYteJOml0rfjffDAwbpovNGOTlh0G+i/GC/K67gBtvdL+vShBW5O5kUZED\neqLu447TBUIM8vKTwVOCXGrdWqvxTz+1P5Fu0hjk7mQV5ADw05/qFsjr1jHIyw0r8l1Mq1bAjBnA\n9dcDu+2WzRgY5O5kGeR9+gBdupR+4mVKHoN8F7P33npS5TR2OQzDIHcnyyAHtCrfc89sx0CNMch3\nMSedBNTVAS1bZjcGth+6k0X7oV///sDbb2f38ykYg3wXs9tuQI8e2Y6BFbk7WVfkInrQk8oLg5wS\nx/ZDd7JqP6TyxiCnxLEidyfripzKE4OcEscgd4dBTkEY5JQ4Brk7DHIKwiCnxLFrxZ2su1aoPDHI\nKXGsyN1hRU5BGOSUOHatuMOuFQrCIKfEsSJ3hxU5BWGQU+IY5O4wyCkIg5wSxyB3h0FOQRjklDh2\nrbjDrhUKwiCnxLEid4cVOQVhkFPi2LXiDrtWKAiDnBLHitwdVuQUhEFOiWOQu8MgpyAMckocg9wd\nBjkFYZBT4hjk7jDIKQiDnBLH9kN32H5IQRjklDh2rbhhDLB9O4OcGmOQU+I4teLG9u1A06Z63kwi\nPwY5JY5B7gbnxykMg5wSxyB3g0FOYRjklDgGuRsMcgrDIKfEsWvFDXasUBirIBeRWhGZKyILROSm\ngO+fLyLTRWSGiPxVRI5MfqhUKViRu8GKnMJEBrmINAUwHEAtgMMADBKRQ/Ou9iGAbxpjjgRwB4BR\nSQ+UKgfbD93ghlkUxqYi7wVgoTFmsTFmG4AxAAb6r2CMmWyMWZ/7ciqATskOkyoJK3I3WJFTGJsg\n7whgqe/rZbnLwvwQwCulDIoqG4PcDQY5hbF5WhjbOxORkwFcAuCkoO8PGTLky3/X1NSgpqbG9q6p\ngjDI3WCQV4e6ujrU1dXFuo0YUzinRaQ3gCHGmNrc1z8FsMMYc2/e9Y4EMBZArTFmYcD9mKifRbuG\nRYuAk08GFi/OeiS7lhkzgPPPB2bOzF0gouv2aZcmIjDGFFzPazO18h6AbiJygIi0AHAugPF5P+hr\n0BC/ICjEqbqwIneDFTmFiXxaGGPqReQqAK8CaArgcWPMHBEZnPv+SAA/A7APgBGiG0FsM8b0cjds\nKmfsWnGDXSsUxur93RgzAcCEvMtG+v59KYBLkx0aVSpW5G6wIqcwXNlJiWOQu8EgpzAMckocg9wN\nBjmFYZBT4rjXihvca4XCMMgpcazI3WBFTmEY5JS4pk2BHTv0DyWHXSsUhkFOiRPRynH79qxHsmth\nRU5hGOTkBKdXkscgpzAMcnKCQZ48BjmFYZCTEwzy5DHIKQyDnJxgC2Ly2H5IYRjk5AT3W0keu1Yo\nDIOcnODUSvI4tUJhGOTkBIM8eQxyCsMgJycY5MljkFMYBjk5wSBPHoOcwjDIyQl2rSSPXSsUhkFO\nTrAiTx4rcgrDICcn2H6YPLYfUhgGOTnBijx5rMgpDIOcnGCQJ49BTmEY5OQEgzx5DHIKwyAnJ9i1\nkjx2rVAYBjk5wYo8eazIKQyDnJxg10ry2LVCYRjk5AQr8uSxIqcwDHJygkGePAY5hWGQkxMM8uQx\nyCkMg5ycYJAnj0FOYRjk5ATbD5PH9kMKwyAnJ9i1kjx2rVAYBjk5wamV5HFqhcIwyMkJBnnyGOQU\nhkFOTjDIk8cgpzAMcnKCQZ48BjmFYZCTE+xaSR67VihMZJCLSK2IzBWRBSJyU8h1Hsp9f7qIHJP8\nMKnSsCJPHityClMwyEWkKYDhAGoBHAZgkIgcmnedMwAcbIzpBuAyACMcjTUVdXV1WQ/BSrmP02s/\nLPdxApUxRgBYv76uItoPK+XxrJRx2oiqyHsBWGiMWWyM2QZgDICBedcZAOBpADDGTAXQRkQ6JD7S\nlFTKL7fcx+lV5OU+TqAyxggAGzfWVURFXimPZ6WM00ZUkHcEsNT39bLcZVHX6VT60KiScWoleTt2\ncGqFgkU9LYzl/UiRt6NdVIsWwO9/r1Msf/971qMpbN688h8jAGzcyJWdFEyMCc9cEekNYIgxpjb3\n9U8B7DDG3Ou7zqMA6owxY3JfzwXQ1xizOu++GO5EREUwxuQXyzuJqsjfA9BNRA4AsALAuQAG5V1n\nPICrAIzJBf+6/BC3GQgRERWnYJAbY+pF5CoArwJoCuBxY8wcERmc+/5IY8wrInKGiCwE8DmAi52P\nmoiIvlRwaoWIiMpf6is7ReR6EdkhIvum/bNtiMgduYVN00TkdRHpnPWYgojIL0RkTm6sY0Vk76zH\nFEREviMi/xSR7SJybNbjyWez4C1rIvKEiKwWkZlZj6UQEeksIm/mft+zROTqrMcURER2F5Gpudf4\nbBG5O+sxhRGRpiLyvoi8XOh6qQZ5LhRPA7AkzZ8b08+NMUcZY44G8AcAt2c9oBCvATjcGHMUgPkA\nfprxeMLMBPBtAG9nPZB8NgveysST0DGWu20AfmyMORxAbwBXluPjaYzZAuDk3Gv8SAAni8g3Mh5W\nmGsAzEZEJ2DaFfn9AG5M+WfGYozZ6PuyJYBPsxpLIcaYScaYHbkvp6JMe/eNMXONMfOzHkcImwVv\nmTPGvANgbdbjiGKMWWWMmZb7978AzAHw1WxHFcwYsyn3zxbQ43+fZTicQCLSCcAZAH6Nxi3eO0kt\nyEVkIIBlxpgZaf3MYonInSLyEYAfALgn6/FYuATAK1kPogLZLHijIuQ63Y6BFhllR0SaiMg0AKsB\nvGmMmZ31mAI8AOAGADuirpjoOjERmQRgv4Bv3Qr96N/ff/Ukf3YcBcZ5izHmZWPMrQBuFZGboQ9m\nJp04UePMXedWAF8YY55NdXA+NuMsUzzS74CItATwIoBrcpV52cl9mj06d2zpVRGpMcbUZTysL4nI\nmQA+Nsa8LyI1UddPNMiNMaeFDOoIAF0BTBcRQKcB/i4ivYwxHyc5Bhth4wzwLDKsdKPGKSIXQT96\n9UtlQCFiPJ7lZjkA/8HsztCqnIokIs0BvARgtDHmD1mPJ4oxZr2I/AnA8QDqMh6OXx8AA3KbEu4O\noLWI/MYYc2HQlVOZWjHGzDLGdDDGdDXGdIW+WI7NIsSjiEg335cDAbyf1VgKEZFa6MeugbmDN5Wg\n3BaFfbngTURaQBe8jc94TBVLtEp7HMBsY8ywrMcTRkTaiUib3L/3gDZglNXr3BhzizGmcy4vvwfg\njbAQB7I7sUQ5f6S9W0Rm5ubPagBcn/F4wjwMPRg7Kdee9KusBxRERL4tIkuhXQx/EpEJWY/JY4yp\nh65KfhXaGfC8MWZOtqNqTESeA/A3AIeIyFIRKddFdycBuADaBfJ+7k85dtvsD+CN3Gt8KoCXjTGv\nZzymKAUzkwuCiIgqHE/1RkRU4RjkREQVjkFORFThGORERBWOQU5EVISkNzMTkYdyG47NFpEH49yW\nQU5EVJzENjPLrd48FsARuT89RaSv7e0Z5ERERQjazExEDhKRCSLynoi8LSLdLe9uNXQDr90A7AGg\nOYBVtmNhkBMRJWcUgB8ZY46Hrry2WqiXW4j2GoCV0K0jJhpj5tn+0ET3WiEiqla5zcJOBPC73J5S\ngFbZEJGzAQwNuNkyY8zpIvJNACdDd98U6IrtV40xf7H52QxyIqJkNIGefP6Y/G8YY8YCGFvgtr0B\nTPD2Sc9tZXEiAKsg59QKEVECjDEbACwSkXMA3URMRI60vPlcAH1zp3ZrDqAvdP8fKwxyIqIi+DYz\n6+7bzOx8AD/Mbcg1C8AAm/syxozPXX86gGkAphlj/mQ9Fm6aRURU2ViRExFVOAY5EVGFY5ATEVU4\nBjkRUYVjkBMRVTgGORFRhWOQExFVOAY5EVGF+3+gbsjxBglfVwAAAABJRU5ErkJggg==\n",
       "text": [
        "<matplotlib.figure.Figure at 0x1129b4650>"
       ]
      }
     ],
     "prompt_number": 28
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# We know from L'Hopital's rule that the answer is 0.5 at 0\n",
      "# and should be very close to 0.5 throughout this tiny interval\n",
      "# but errors arisee due to catastrophic cancellation\n",
      "\n",
      "print '%.30f' % np.cos(1.1e-8)\n",
      "print '%.30f' % (1 - np.cos(1.1e-8)) # exact answer is 6.05e-17\n",
      "print '%2f' % ((1 - np.cos(1.1e-8))/(1.1e-8*1.1e-8))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.999999999999999888977697537484\n",
        "0.000000000000000111022302462516\n",
        "0.917540\n"
       ]
      }
     ],
     "prompt_number": 29
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Stable version**"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Numerically stable version of funtion\n",
      "# using long-forgotten half-angle formula from trignometry\n",
      "\n",
      "def f1(x):\n",
      "    return 2*np.sin(x/2)**2/(x*x)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 30
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "x = np.linspace(-4e-8, 4e-8, 100)\n",
      "plt.plot(x,f1(x));\n",
      "plt.axvline(1.1e-8, color='red')\n",
      "plt.xlim([-4e-8, 4e-8]);"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAXgAAAENCAYAAAAFcn7UAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\napQBL0mNMuAlqVEGvCQ16v8BgRRSflFxeW4AAAAASUVORK5CYII=\n",
       "text": [
        "<matplotlib.figure.Figure at 0x1134e9fd0>"
       ]
      }
     ],
     "prompt_number": 31
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Stable and unstable versions of variance"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "$$s^2 = \\frac{1}{n-1}\\sum(x - \\bar{x})^2$$"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# sum of squares method (vectorized version)\n",
      "def sum_of_squers_var(x):\n",
      "    n = len(x)\n",
      "    return (1.0/(n*(n-1))*(n*np.sum(x**2) - (np.sum(x))**2))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 32
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<font color=blue>This should set off warning bells - big number minus big number!</font>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# direct method\n",
      "def direct_var(x):\n",
      "    n = len(x)\n",
      "    xbar = np.mean(x)\n",
      "    return 1.0/(n-1)*np.sum((x - xbar)**2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 33
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<font color=blue>Much better - at least the squaring occurs after the subtraction</font>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Welford's method\n",
      "def welford_var(x):\n",
      "    s = 0\n",
      "    m = x[0]\n",
      "    for i in range(1, len(x)):\n",
      "        m += (x[i]-m)/i\n",
      "        s += (x[i]-m)**2\n",
      "    return s/(len(x) -1 )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 34
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<font color=blue>Classic algorithm from Knuth's Art of Computer Programming</font>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "x_ = np.random.uniform(0,1,1e6)\n",
      "x = 1e12 + x_"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 35
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# correct answer\n",
      "np.var(x_)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 36,
       "text": [
        "0.0835"
       ]
      }
     ],
     "prompt_number": 36
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sum_of_squers_var(x)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 37,
       "text": [
        "737870500.8189"
       ]
      }
     ],
     "prompt_number": 37
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "direct_var(x)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 38,
       "text": [
        "0.0835"
       ]
      }
     ],
     "prompt_number": 38
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "welford_var(x)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 39,
       "text": [
        "0.0835"
       ]
      }
     ],
     "prompt_number": 39
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<font color=blue>Lesson: Mathematical formulas may behave differently when directly translated into code!</font>\n",
      "\n",
      "This problem also appears in navie algorithms for finding simple regression coefficients and Pearson\u2019s correlation coefficient."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Poorly conditioned problems"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# The tangent function is poorly conditioned\n",
      "\n",
      "x1 = 1.57078\n",
      "x2 = 1.57079\n",
      "t1 = np.tan(x1)\n",
      "t2 = np.tan(x2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 40
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print 't1 =', t1\n",
      "print 't2 =', t2\n",
      "print '% change in x =', 100.0*(x2-x1)/x1\n",
      "print '% change in tan(x) =', (100.0*(t2-t1)/t1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "t1 = 61249.0085315\n",
        "t2 = 158057.913416\n",
        "% change in x = 0.000636626389427\n",
        "% change in tan(x) = 158.057913435\n"
       ]
      }
     ],
     "prompt_number": 41
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Ill-conditioned matrices**"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In this example, we want to solve a simple linear system Ax = b\n",
      "where A and b are given."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "A = 0.5*np.array([[1,1], [1+1e-10, 1-1e-10]])\n",
      "b1 = np.array([2,2])\n",
      "b2 = np.array([2.01, 2])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 42
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "np.linalg.solve(A, b1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 43,
       "text": [
        "array([ 2.,  2.])"
       ]
      }
     ],
     "prompt_number": 43
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "np.linalg.solve(A, b2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 44,
       "text": [
        "array([-99999989.706,  99999993.726])"
       ]
      }
     ],
     "prompt_number": 44
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The condition number of a matrix is a useful diagnostic - it is defined as the norm of A times the norm of the inverse of A. If this number is large, the matrix is ill-conditioned. Since there are many ways to calculuate a matrix norm, there are also many condition numbers, but they are roughly equivalent for our purpsoes."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "np.linalg.cond(A)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 45,
       "text": [
        "19999973849.2252"
       ]
      }
     ],
     "prompt_number": 45
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "np.linalg.cond(A, 'fro')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 46,
       "text": [
        "19999998343.1927"
       ]
      }
     ],
     "prompt_number": 46
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<font color=blue>Lesson: Accuracy depends on both stability and conditioning.</font>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### Simple things to try with ill-conditioned matrices\n",
      "\n",
      "- Can you remove dependent or collinear variables? If one variable is (almost) an exact muliple of another, it provides no additional information and can be removed from the matrix.\n",
      "- Can you normalize the data so that all vairables are on the same scale? For example, if columns represent featrue values, standardizign featurres to have zero mean and unit standard deviaiton can be helpful.\n",
      "- Can you use functions from linear algebra libraries instead of rolling your own. For example, the `lstsq` function from `scipy.linalg` will deal with collinear variables sensibly.\n",
      "\n",
      "#### Less simple thingss\n",
      "\n",
      "- You can \"precondition\" a matrix to reduce its condition number. See [Wikipedia](http://en.wikipedia.org/wiki/Preconditioner)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Collinear variables\n",
      "\n",
      "n, p = 10, 6\n",
      "\n",
      "A = np.random.random((n,p))\n",
      "A[:, -1] = A[:, 0] # create dependent column\n",
      "x = np.random.random(p).reshape((p, 1))\n",
      "x[-1] = x[0]\n",
      "b = np.dot(A, x)\n",
      "\n",
      "import scipy.linalg as la\n",
      "\n",
      "xhat = la.lstsq(A, b)[0]\n",
      "print x.T\n",
      "print xhat.T\n",
      "\n",
      "# roll your own version using $A^TA x = A^Tb$\n",
      "# This may either fail becuas the matix is singular \n",
      "# or give erroneous results (matrix is not singular due \n",
      "# to floaing point inaccuracies)\n",
      "try:\n",
      "    xhat1 = la.solve(np.dot(A.T, A), np.dot(A.T, b))\n",
      "    print xhat1.T\n",
      "except la.LinAlgError:\n",
      "    print \"Singluar matrix\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[[ 0.8464  0.8527  0.6134  0.3143  0.4506  0.8464]]\n",
        "[[ 0.8464  0.8527  0.6134  0.3143  0.4506  0.8464]]\n",
        "Singluar matrix\n"
       ]
      }
     ],
     "prompt_number": 52
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 48
    }
   ],
   "metadata": {}
  }
 ]
}