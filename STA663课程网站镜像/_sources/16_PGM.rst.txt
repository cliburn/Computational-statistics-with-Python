
Probabilistic Graphical Models with ``pgmpy``
=============================================

.. code:: python

    !pip install pgmpy


.. parsed-literal::

    Collecting pgmpy
      Downloading pgmpy-0.1.2.tar.gz (147kB)
    [K    100% |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 153kB 5.9MB/s 
    [?25hRequirement already satisfied (use --upgrade to upgrade): networkx>=1.8.1 in /opt/conda/lib/python3.5/site-packages (from pgmpy)
    Requirement already satisfied (use --upgrade to upgrade): scipy>=0.12.1 in /opt/conda/lib/python3.5/site-packages (from pgmpy)
    Requirement already satisfied (use --upgrade to upgrade): numpy>=1.7.0 in /opt/conda/lib/python3.5/site-packages (from pgmpy)
    Requirement already satisfied (use --upgrade to upgrade): nose>=1.3.0 in /opt/conda/lib/python3.5/site-packages (from pgmpy)
    Collecting coveralls>=0.4 (from pgmpy)
      Downloading coveralls-1.1-py2.py3-none-any.whl
    Requirement already satisfied (use --upgrade to upgrade): decorator>=3.4.0 in /opt/conda/lib/python3.5/site-packages (from networkx>=1.8.1->pgmpy)
    Collecting docopt>=0.6.1 (from coveralls>=0.4->pgmpy)
      Downloading docopt-0.6.2.tar.gz
    Collecting coverage>=3.6 (from coveralls>=0.4->pgmpy)
      Downloading coverage-4.3.4-cp35-cp35m-manylinux1_x86_64.whl (191kB)
    [K    100% |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 194kB 4.7MB/s 
    [?25hRequirement already satisfied (use --upgrade to upgrade): requests>=1.0.0 in /opt/conda/lib/python3.5/site-packages (from coveralls>=0.4->pgmpy)
    Building wheels for collected packages: pgmpy, docopt
      Running setup.py bdist_wheel for pgmpy ... [?25l- \ | done
    [?25h  Stored in directory: /home/jovyan/.cache/pip/wheels/d3/21/0f/5b1fc282ee2ab16b693c1a0ed9cb8fde44dbaa28d907c90ff4
      Running setup.py bdist_wheel for docopt ... [?25l- \ done
    [?25h  Stored in directory: /home/jovyan/.cache/pip/wheels/b2/16/5f/c33a2bb5f2dce71205f8e65cbfd05647d79d441282be31fd82
    Successfully built pgmpy docopt
    Installing collected packages: docopt, coverage, coveralls, pgmpy
    Successfully installed coverage-4.3.4 coveralls-1.1 docopt-0.6.2 pgmpy-0.1.2
    [33mYou are using pip version 8.1.2, however version 9.0.1 is available.
    You should consider upgrading via the 'pip install --upgrade pip' command.[0m


.. code:: python

    from pgmpy.factors import TabularCPD

.. code:: python

    # Declare a CPD 
    grade_cpd = TabularCPD(variable="G",
        variable_card=3,
        values=[[0.3, 0.05, 0.9, 0.5],
        [0.4, 0.25, 0.08, 0.3],
        [0.3, 0.7, 0.02, 0.2]],
        evidence=["I", "D"],
        evidence_card=[2, 2])
    grade_cpd




.. raw:: html

    <table><caption>TabularCPD for <b>G</b></caption><tr><td><b>D</b></td><td colspan=2>D_0</td><td colspan=2>D_1</td></tr><tr><td><b>I</b></td><td colspan=1>I_0</td><td colspan=1>I_1</td><td colspan=1>I_0</td><td colspan=1>I_1</td></tr><tr><td><b>G_0</b></td><td>0.3000</td><td>0.0500</td><td>0.9000</td><td>0.5000</td></tr><tr><td><b>G_1</b></td><td>0.4000</td><td>0.2500</td><td>0.0800</td><td>0.3000</td></tr><tr><td><b>G_2</b></td><td>0.3000</td><td>0.7000</td><td>0.0200</td><td>0.2000</td></tr></table>



.. code:: python

    # Declare the sudent model in pgmpy
    
    from pgmpy.models import BayesianModel
    from pgmpy.factors import TabularCPD
    
    # Define nodes and edges
    student_model = BayesianModel([("D", "G"),
    ("I", "G"),
    ("G", "L"),
    ("I", "S")])
    
    #Define CPDs
    
    grade_cpd = TabularCPD(
    variable="G",
    variable_card=3,
    values=[[0.3, 0.05, 0.9, 0.5],
    [0.4, 0.25, 0.08, 0.3],
    [0.3, 0.7, 0.02, 0.2]],
    evidence=["I", "D"],
    evidence_card=[2, 2])
    
    difficulty_cpd = TabularCPD(
    variable="D",
    variable_card=2,
    values=[[0.6, 0.4]])
    
    intel_cpd = TabularCPD(
    variable="I",
    variable_card=2,
    values=[[0.7, 0.3]])
    
    letter_cpd = TabularCPD(
    variable="L",
    variable_card=2,
    values=[[0.1, 0.4, 0.99],
    [0.9, 0.6, 0.01]],
    evidence=["G"],
    evidence_card=[3])
    
    sat_cpd = TabularCPD(
    variable="S",
    variable_card=2,
    values=[[0.95, 0.2],
    [0.05, 0.8]],
    evidence=["I"],
    evidence_card=[2])
    
    
    #Add CPDs to nodes and edges
    student_model.add_cpds(grade_cpd, difficulty_cpd,
    intel_cpd, letter_cpd,
    sat_cpd)
    
    grade_cpd




.. raw:: html

    <table><caption>TabularCPD for <b>G</b></caption><tr><td><b>D</b></td><td colspan=2>D_0</td><td colspan=2>D_1</td></tr><tr><td><b>I</b></td><td colspan=1>I_0</td><td colspan=1>I_1</td><td colspan=1>I_0</td><td colspan=1>I_1</td></tr><tr><td><b>G_0</b></td><td>0.3000</td><td>0.0500</td><td>0.9000</td><td>0.5000</td></tr><tr><td><b>G_1</b></td><td>0.4000</td><td>0.2500</td><td>0.0800</td><td>0.3000</td></tr><tr><td><b>G_2</b></td><td>0.3000</td><td>0.7000</td><td>0.0200</td><td>0.2000</td></tr></table>



.. code:: python

    student_model.get_cpds('G')





.. raw:: html

    <table><caption>TabularCPD for <b>G</b></caption><tr><td><b>D</b></td><td colspan=2>D_0</td><td colspan=2>D_1</td></tr><tr><td><b>I</b></td><td colspan=1>I_0</td><td colspan=1>I_1</td><td colspan=1>I_0</td><td colspan=1>I_1</td></tr><tr><td><b>G_0</b></td><td>0.3000</td><td>0.0500</td><td>0.9000</td><td>0.5000</td></tr><tr><td><b>G_1</b></td><td>0.4000</td><td>0.2500</td><td>0.0800</td><td>0.3000</td></tr><tr><td><b>G_2</b></td><td>0.3000</td><td>0.7000</td><td>0.0200</td><td>0.2000</td></tr></table>



.. code:: python

    student_model.get_parents('G')




.. parsed-literal::

    ['D', 'I']



.. code:: python

    from pgmpy.inference import VariableElimination
    
    student_infer = VariableElimination(student_model)
    
    prob_G = student_infer.query(variables='G')
    
    print(prob_G['G'])


.. parsed-literal::

    â•’â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•â•â•â••
    â”‚ G   â”‚   phi(G) â”‚
    â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•¡
    â”‚ G_0 â”‚   0.4470 â”‚
    â”œâ”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚ G_1 â”‚   0.2714 â”‚
    â”œâ”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚ G_2 â”‚   0.2816 â”‚
    â•˜â•â•â•â•â•â•§â•â•â•â•â•â•â•â•â•â•â•›


.. code:: python

    prob_G = student_infer.query(variables='G', evidence={'I': 1, 'D' : 0})
    
    print(prob_G['G'])


.. parsed-literal::

    â•’â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•â•â•â••
    â”‚ G   â”‚   phi(G) â”‚
    â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•¡
    â”‚ G_0 â”‚   0.0500 â”‚
    â”œâ”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚ G_1 â”‚   0.2500 â”‚
    â”œâ”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚ G_2 â”‚   0.7000 â”‚
    â•˜â•â•â•â•â•â•§â•â•â•â•â•â•â•â•â•â•â•›


.. code:: python

    prob_G = student_infer.query(variables='G', evidence={'I': 0, 'D' : 1})
    
    print(prob_G['G'])


.. parsed-literal::

    â•’â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•â•â•â••
    â”‚ G   â”‚   phi(G) â”‚
    â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•¡
    â”‚ G_0 â”‚   0.9000 â”‚
    â”œâ”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚ G_1 â”‚   0.0800 â”‚
    â”œâ”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚ G_2 â”‚   0.0200 â”‚
    â•˜â•â•â•â•â•â•§â•â•â•â•â•â•â•â•â•â•â•›


.. code:: python

    #Train Model from Data
    
    from pgmpy.models import BayesianModel
    import pandas as pd
    import numpy as np
    # Considering that each variable have only 2 states,
    # we can generate some random data.
    
    raw_data = np.random.randint(low=0,high=2,size=(1000, 5))
    
    
    data = pd.DataFrame(raw_data,columns=["D", "I", "G","L", "S"])
    
    print(data[: int(data.shape[0]*0.75)])
    
    data_train = data[: int(data.shape[0] * 0.75)]
    
    student_model = BayesianModel([("D", "G"),("I", "G"),("I", "S"),("G", "L")])
    student_model.fit(data_train)
    student_model.get_cpds('D')


.. parsed-literal::

         D  I  G  L  S
    0    0  1  1  1  1
    1    0  0  0  0  1
    2    0  1  1  0  0
    3    0  0  0  0  1
    4    1  1  0  1  1
    5    1  0  0  0  0
    6    1  1  0  1  1
    7    1  0  0  0  1
    8    1  1  0  0  1
    9    1  0  0  0  0
    10   1  1  1  1  0
    11   1  1  0  0  0
    12   1  1  1  1  0
    13   1  0  0  1  1
    14   0  1  1  1  1
    15   1  0  0  1  1
    16   1  1  0  1  1
    17   0  0  0  1  0
    18   0  0  0  0  0
    19   1  1  1  1  1
    20   0  0  0  1  1
    21   0  0  0  0  0
    22   0  0  1  0  0
    23   0  0  0  1  0
    24   0  0  1  1  1
    25   1  0  1  1  1
    26   0  1  1  1  0
    27   0  1  0  0  1
    28   1  1  0  0  1
    29   1  0  0  0  0
    ..  .. .. .. .. ..
    720  1  1  1  0  1
    721  0  0  1  1  1
    722  1  1  1  0  1
    723  1  0  0  0  0
    724  1  0  0  1  1
    725  0  0  1  1  1
    726  0  0  1  0  1
    727  0  0  1  0  0
    728  1  1  1  0  0
    729  1  1  0  0  0
    730  0  0  0  0  0
    731  1  1  1  0  0
    732  0  0  1  0  1
    733  1  1  0  1  0
    734  0  1  0  1  1
    735  0  1  1  1  0
    736  1  0  1  0  0
    737  1  1  1  1  0
    738  1  1  1  1  1
    739  0  1  1  0  0
    740  1  1  1  1  1
    741  1  0  1  1  0
    742  0  1  1  0  1
    743  1  0  1  1  0
    744  0  1  1  1  1
    745  1  0  0  0  1
    746  1  0  0  1  0
    747  0  1  1  0  1
    748  1  0  0  1  1
    749  0  0  1  1  1
    
    [750 rows x 5 columns]




.. raw:: html

    <table><caption>TabularCPD for <b>D</b></caption><tr><td><b>D_0</b></td><td>0.4400</td></tr><tr><td><b>D_1</b></td><td>0.5600</td></tr></table>



.. code:: python

    student_model.get_cpds('L')




.. raw:: html

    <table><caption>TabularCPD for <b>L</b></caption><tr><td><b>G</b></td><td colspan=1>G_0</td><td colspan=1>G_1</td></tr><tr><td><b>L_0</b></td><td>0.4545</td><td>0.5000</td></tr><tr><td><b>L_1</b></td><td>0.5455</td><td>0.5000</td></tr></table>



.. code:: python

    student_model.active_trail_nodes('D')





.. parsed-literal::

    {'D', 'G', 'L'}



.. code:: python

    student_model.local_independencies('G')





.. parsed-literal::

    (G _|_ S | D, I)



.. code:: python

    student_model.get_independencies()




.. parsed-literal::

    (G _|_ L, I, S | D)
    (G _|_ L, I, D | S)
    (G _|_ I, S, D | L)
    (G _|_ L, D | I)
    (D _|_ I, S | G)
    (D _|_ G, L | S)
    (D _|_ G, I, S | L)
    (D _|_ G, L | I)
    (S _|_ I, D | G)
    (S _|_ G, I, L | D)
    (S _|_ G, I, D | L)
    (L _|_ G, I, S | D)
    (L _|_ G, I, D | S)
    (L _|_ G, D | I)
    (I _|_ D, S | G)
    (I _|_ G, S, L | D)
    (I _|_ G, L | S)
    (I _|_ G, D, S | L)



.. code:: python

    data_test = data[int(0.75 * data.shape[0]) : data.shape[0]]
    
    data_test.drop('G', axis=1, inplace=True)
    
    student_model.predict(data_test)


.. parsed-literal::

    /opt/conda/lib/python3.5/site-packages/ipykernel/__main__.py:3: SettingWithCopyWarning: 
    A value is trying to be set on a copy of a slice from a DataFrame
    
    See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy
      app.launch_new_instance()




.. raw:: html

    <div>
    <table border="1" class="dataframe">
      <thead>
        <tr style="text-align: right;">
          <th></th>
          <th>G</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <th>750</th>
          <td>1</td>
        </tr>
        <tr>
          <th>751</th>
          <td>0</td>
        </tr>
        <tr>
          <th>752</th>
          <td>0</td>
        </tr>
        <tr>
          <th>753</th>
          <td>0</td>
        </tr>
        <tr>
          <th>754</th>
          <td>0</td>
        </tr>
        <tr>
          <th>755</th>
          <td>0</td>
        </tr>
        <tr>
          <th>756</th>
          <td>0</td>
        </tr>
        <tr>
          <th>757</th>
          <td>1</td>
        </tr>
        <tr>
          <th>758</th>
          <td>0</td>
        </tr>
        <tr>
          <th>759</th>
          <td>1</td>
        </tr>
        <tr>
          <th>760</th>
          <td>0</td>
        </tr>
        <tr>
          <th>761</th>
          <td>0</td>
        </tr>
        <tr>
          <th>762</th>
          <td>1</td>
        </tr>
        <tr>
          <th>763</th>
          <td>1</td>
        </tr>
        <tr>
          <th>764</th>
          <td>0</td>
        </tr>
        <tr>
          <th>765</th>
          <td>1</td>
        </tr>
        <tr>
          <th>766</th>
          <td>1</td>
        </tr>
        <tr>
          <th>767</th>
          <td>0</td>
        </tr>
        <tr>
          <th>768</th>
          <td>0</td>
        </tr>
        <tr>
          <th>769</th>
          <td>1</td>
        </tr>
        <tr>
          <th>770</th>
          <td>1</td>
        </tr>
        <tr>
          <th>771</th>
          <td>1</td>
        </tr>
        <tr>
          <th>772</th>
          <td>0</td>
        </tr>
        <tr>
          <th>773</th>
          <td>1</td>
        </tr>
        <tr>
          <th>774</th>
          <td>1</td>
        </tr>
        <tr>
          <th>775</th>
          <td>0</td>
        </tr>
        <tr>
          <th>776</th>
          <td>1</td>
        </tr>
        <tr>
          <th>777</th>
          <td>1</td>
        </tr>
        <tr>
          <th>778</th>
          <td>0</td>
        </tr>
        <tr>
          <th>779</th>
          <td>0</td>
        </tr>
        <tr>
          <th>...</th>
          <td>...</td>
        </tr>
        <tr>
          <th>970</th>
          <td>1</td>
        </tr>
        <tr>
          <th>971</th>
          <td>0</td>
        </tr>
        <tr>
          <th>972</th>
          <td>1</td>
        </tr>
        <tr>
          <th>973</th>
          <td>1</td>
        </tr>
        <tr>
          <th>974</th>
          <td>1</td>
        </tr>
        <tr>
          <th>975</th>
          <td>0</td>
        </tr>
        <tr>
          <th>976</th>
          <td>1</td>
        </tr>
        <tr>
          <th>977</th>
          <td>0</td>
        </tr>
        <tr>
          <th>978</th>
          <td>1</td>
        </tr>
        <tr>
          <th>979</th>
          <td>1</td>
        </tr>
        <tr>
          <th>980</th>
          <td>1</td>
        </tr>
        <tr>
          <th>981</th>
          <td>1</td>
        </tr>
        <tr>
          <th>982</th>
          <td>1</td>
        </tr>
        <tr>
          <th>983</th>
          <td>0</td>
        </tr>
        <tr>
          <th>984</th>
          <td>1</td>
        </tr>
        <tr>
          <th>985</th>
          <td>1</td>
        </tr>
        <tr>
          <th>986</th>
          <td>1</td>
        </tr>
        <tr>
          <th>987</th>
          <td>0</td>
        </tr>
        <tr>
          <th>988</th>
          <td>0</td>
        </tr>
        <tr>
          <th>989</th>
          <td>1</td>
        </tr>
        <tr>
          <th>990</th>
          <td>0</td>
        </tr>
        <tr>
          <th>991</th>
          <td>0</td>
        </tr>
        <tr>
          <th>992</th>
          <td>0</td>
        </tr>
        <tr>
          <th>993</th>
          <td>0</td>
        </tr>
        <tr>
          <th>994</th>
          <td>0</td>
        </tr>
        <tr>
          <th>995</th>
          <td>1</td>
        </tr>
        <tr>
          <th>996</th>
          <td>1</td>
        </tr>
        <tr>
          <th>997</th>
          <td>1</td>
        </tr>
        <tr>
          <th>998</th>
          <td>0</td>
        </tr>
        <tr>
          <th>999</th>
          <td>0</td>
        </tr>
      </tbody>
    </table>
    <p>250 rows Ã— 1 columns</p>
    </div>


